# Copyright (c) 2025-2026, NVIDIA CORPORATION.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#===============================================================================
# RAPIDS Accelerator Tuning Definitions Table
#===============================================================================
# This YAML file defines tuning property definitions used by the RAPIDS
# Accelerator for Apache Spark tools. It provides metadata about Spark
# configuration properties that can be tuned for optimal GPU performance.
#
# Purpose:
# - Defines properties that the AutoTuner can recommend and configure
# - Provides descriptions, categories, and default values for each property
# - Specifies data types and units for proper validation and formatting
# - Contains plugin-specific properties for technologies like Delta Lake
#
# Structure:
# The file contains a single top-level key 'tuningDefinitions' which is a list
# of tuning property definitions. Each definition describes a Spark configuration
# property with the following fields:
#
# Required Fields:
# - label:       The Spark configuration property name (e.g., "spark.executor.cores")
# - description: Human-readable description of what this property does
# - enabled:     Whether this property is active for tuning (true/false)
# - level:       Scope of the property - either "cluster" or "job"
#                * cluster: Applied at cluster/executor level (e.g., memory, cores)
#                * job: Applied at job/query level (e.g., AQE settings, shuffle)
# - category:    Type of property - either "tuning" or "functionality"
#                * tuning: Performance optimization settings
#                * functionality: Features that enable/configure GPU capabilities
# - confType:    Data type information for the property
#   - name:      Type name (boolean, int, long, double, string, byte, time)
#   - defaultUnit: (Optional) Default unit for numeric types (e.g., MiB, Byte, ms)
#
# Optional Fields:
# - defaultSpark: The default value in vanilla Apache Spark (if different from RAPIDS)
# - defaultMemoryUnit: (Deprecated) Unit for memory values - use confType.defaultUnit instead
# - modifiedBy:  Name of the plugin or rule that modifies this property
# - comments:    Map of comment types to messages shown to users
#   - missing:     Message when property is not set but should be
#   - updated:     Message when property exists but should be modified
#   - persistent:  Message that always appears for this property
#
# Configuration Type Details:
# The 'confType' field specifies how to parse and validate property values:
# - boolean:  true/false values
# - int:      Integer numbers
# - long:     Long integer numbers
# - double:   Floating point numbers
# - string:   Text values
# - byte:     Memory sizes (supports units like MiB, GiB, Byte, etc.)
# - time:     Time durations (supports units like ms, s, m, h, etc.)
#
# Organization:
# 1. General Spark Properties (lines ~50-250)
#    - Executor configuration (cores, memory, overhead)
#    - Serialization settings (Kryo)
#    - AQE (Adaptive Query Execution) properties
#    - Shuffle and partition settings
#    - GPU resource scheduling
#
# 2. RAPIDS Plugin Properties (lines ~250-400)
#    - RAPIDS-specific acceleration settings
#    - File cache, pinned memory, batch sizes
#    - Multi-threaded readers and shuffle settings
#
# 3. Plugin-Specific Properties (lines ~400+)
#    - Delta Lake optimizations and acceleration
#    - Technology-specific tuning recommendations
#    - Version-specific workarounds or settings
#
# Usage:
# This file is loaded by the AutoTuner to understand which properties can be
# recommended, what their valid ranges are, and how to format their values.
# It works in conjunction with tuningConfigs.yaml which provides the actual
# calculation logic for determining recommended values.
#
# Note:
# Some properties are marked with 'modifiedBy' to indicate they are controlled
# by specific tuning plugins or rules. These plugins may apply conditional logic
# based on the environment (e.g., Delta Lake version, platform type).
#===============================================================================

tuningDefinitions:
  - label: spark.driver.extraJavaOptions
    description: >-
      Extra JVM options to pass to the driver.
      On EMR, this is used to disable Transparent Huge Pages (THP) for improved performance.
    enabled: true
    level: cluster
    category: tuning
    modifiedBy: EmrThpDriverRule
    confType:
      name: string
  - label: spark.executor.extraJavaOptions
    description: >-
      Extra JVM options to pass to executors.
      On EMR, this is used to disable Transparent Huge Pages (THP) for improved performance.
    enabled: true
    level: cluster
    category: tuning
    modifiedBy: EmrThpExecutorRule
    confType:
      name: string
  - label: spark.databricks.adaptive.autoOptimizeShuffle.enabled
    description: >-
      Auto-Optimized shuffle. It is recommended to turn it off to set 'spark.sql.shuffle.partitions' manually.
    enabled: true
    level: job
    category: tuning
    confType:
      name: boolean
  - label: spark.dataproc.enhanced.execution.enabled
    description: Enables enhanced execution. Turning this on might cause the accelerated dataproc cluster to hang.
    enabled: true
    level: job
    category: tuning
    confType:
      name: boolean
    comments:
      persistent: >-
        should be disabled. WARN: Turning this property on might case the GPU accelerated Dataproc cluster to hang.
  - label: spark.dataproc.enhanced.optimizer.enabled
    description: Enables enhanced optimizer. Turning this on might cause the accelerated dataproc cluster to hang.
    enabled: true
    level: job
    category: tuning
    confType:
      name: boolean
    comments:
      persistent: >-
        should be disabled. WARN: Turning this property on might case the GPU accelerated Dataproc cluster to hang.
  - label: spark.executor.cores
    description: The number of cores to use on each executor. It is recommended to be set to 16.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: int
  - label: spark.executor.instances
    description: >-
      Controls parallelism level. It is recommended to be set to (cpuCoresPerNode * numWorkers) / spark.executor.cores.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: int
  - label: spark.executor.memory
    description: >-
      Amount of memory to use per executor process. This is tuned based on the available CPU memory on worker node.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: byte
      defaultUnit: MiB
  - label: spark.executor.memoryOverhead
    description: >-
      Amount of additional memory to be allocated per executor process, in MiB unless otherwise specified.
      This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc.
      This tends to grow with the executor size.
      Note: memoryOverheadFactor is not recommended as specifying the overhead explicitly is sufficient.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: byte
      defaultUnit: MiB
  - label: spark.kryo.registrator
    description: >-
      Fraction of executor memory to be allocated as additional non-heap memory per executor process.
      This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc.
      This tends to grow with the container size.
    enabled: true
    level: job
    category: functionality
    confType:
      name: string
    comments:
      missing: should include GpuKryoRegistrator when using Kryo serialization.
      updated: GpuKryoRegistrator must be appended to the existing value when using Kryo serialization.
  - label: spark.kryoserializer.buffer.max
    description: >-
      This property helps manage the buffer used by Kryo during serialization, preventing out-of-memory
      errors when dealing with large objects.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "64MB"
    confType:
      name: byte
      defaultUnit: MiB
    comments:
      missing: setting the max buffer to prevent out-of-memory errors.
      updated: increasing the max buffer to prevent out-of-memory errors.
  - label: spark.locality.wait
    description: >-
      The time to wait to launch a data-local task before giving up and launching it on a less-local node.
      It is recommended to avoid waiting for a data-local task.
    enabled: true
    level: cluster
    category: tuning
    defaultSpark: "3s"
    confType:
      name: time
      unit: "ms"
  - label: spark.rapids.filecache.enabled
    description: >-
      Enables RAPIDS file cache. The file cache stores data locally in the same local directories
      that have been configured for the Spark executor.
    enabled: true
    level: job
    category: tuning
    confType:
      name: boolean
  - label: spark.rapids.memory.pinnedPool.size
    description: The size of the pinned memory pool in bytes unless otherwise specified. Use 0 to disable the pool.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: byte
      defaultUnit: Byte
  - label: spark.rapids.shuffle.multiThreaded.maxBytesInFlight
    description: >-
      This property controls the amount of bytes we allow in flight per Spark task.
      This typically happens on the reader side, when blocks are received from the network,
      they're queued onto these threads for decompression and decode.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: byte
      defaultUnit: Byte
  - label: spark.rapids.shuffle.multiThreaded.reader.threads
    description: >-
      The shuffle reader is a single implementation irrespective of the number of partitions.
      Set the value to zero to turn off multi-threaded reader entirely.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: int
  - label: spark.rapids.shuffle.multiThreaded.writer.threads
    description: >-
      Controls the number of threads used for writing shuffle data in a multi-threaded shuffle writer
      for the Rapids shuffle implementation.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: int
  - label: spark.rapids.sql.batchSizeBytes
    description: >-
      Set the target number of bytes for a GPU batch. Splits sizes for input data is covered by separate configs.
    enabled: true
    level: job
    category: tuning
    confType:
      name: byte
      defaultUnit: Byte
  - label: spark.rapids.sql.concurrentGpuTasks
    description: >-
      Set the number of tasks that can execute concurrently per GPU. Tasks may temporarily block when the number of
      concurrent tasks in the executor exceeds this amount. Allowing too many concurrent tasks on the same GPU may
      lead to GPU out of memory errors.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: int
  - label: spark.rapids.sql.format.parquet.multithreaded.combine.waitTime
    description: >-
      When using the multithreaded parquet reader with combine mode, how long to wait, in milliseconds,
      for more files to finish if haven't met the size threshold. Note that this will wait this amount
      of time from when the last file was available, so total wait time could be larger then this.
      DEPRECATED: use spark.rapids.sql.reader.multithreaded.combine.waitTime instead.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: int
  - label: spark.rapids.sql.enabled
    description: Should be true to enable SQL operations on the GPU.
    enabled: true
    level: cluster
    category: functionality
    confType:
      name: boolean
  - label: spark.rapids.sql.multiThreadedRead.numThreads
    description: The maximum number of threads on each executor to use for reading small files in parallel.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: int
  - label: spark.rapids.sql.reader.multithreaded.combine.sizeBytes
    description: >-
      The target size in bytes to combine multiple small files together when using the MULTITHREADED
      parquet or orc reader. With combine disabled, the MULTITHREADED reader reads the files in parallel
      and sends individual files down to the GPU, but that can be inefficient for small files.
    enabled: true
    level: job
    category: tuning
    confType:
      name: byte
      defaultUnit: Byte
  - label: spark.serializer
    description: >-
      Specifies which serialization mechanism to use when serializing objects during distributed computation.
      This impacts performance, memory usage, and efficiency in data shuffling and caching.
      When set to Kryo, then the GPU configuration has to include GpuKryoRegistrator for the kryo registrator property.
    enabled: true
    level: job
    category: tuning
    confType:
      name: string
  - label: spark.shuffle.manager
    description: >-
      The RAPIDS Shuffle Manager is an implementation of the ShuffleManager interface in Apache Spark that
      allows custom mechanisms to exchange shuffle data.
      We currently expose two modes of operation; Multi Threaded and UCX.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: string
  - label: spark.sql.adaptive.enabled
    description: >-
      When true, enable adaptive query execution, which re-optimizes the query plan in the middle of
      query execution, based on accurate runtime statistics.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "true"
    confType:
      name: boolean
  - label: spark.sql.adaptive.advisoryPartitionSizeInBytes
    description: >-
      The advisory size in bytes of the shuffle partition during adaptive optimization
      (when 'spark.sql.adaptive.enabled' is true).
      It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.
    enabled: true
    level: job
    category: tuning
    defaultMemoryUnit: Byte
    confType:
      name: byte
      defaultUnit: Byte
  - label: spark.sql.adaptive.coalescePartitions.initialPartitionNum
    description: >-
      The initial number of shuffle partitions before coalescing. If not set, it equals to
      'spark.sql.shuffle.partitions'.
    enabled: true
    level: job
    category: tuning
    confType:
      name: int
  - label: spark.sql.adaptive.coalescePartitions.minPartitionNum
    description: >-
      (deprecated) The suggested (not guaranteed) minimum number of shuffle partitions after coalescing.
      If not set, the default value is the default parallelism of the Spark cluster.
    enabled: true
    level: job
    category: tuning
    confType:
      name: int
  - label: spark.sql.adaptive.coalescePartitions.minPartitionSize
    description: >-
      The minimum size of shuffle partitions after coalescing. This is useful when the adaptively calculated
      target size is too small during partition coalescing.
    enabled: true
    level: job
    category: tuning
    defaultSpark: 1m
    defaultMemoryUnit: Byte
    confType:
      name: byte
      defaultUnit: Byte
  - label: spark.sql.adaptive.coalescePartitions.parallelismFirst
    description: >-
      When true, Spark does not respect the target size specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'
      (default 64MB) when coalescing contiguous shuffle partitions, but adaptively calculate the target size
      according to the default parallelism of the Spark cluster.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "true"
    confType:
      name: boolean
  - label: spark.sql.adaptive.autoBroadcastJoinThreshold
    description: >-
      Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when
      performing a join. By setting this value to -1, broadcasting can be disabled.
    enabled: true
    level: job
    category: tuning
    defaultMemoryUnit: Byte
    confType:
      name: byte
      defaultUnit: Byte
  - label: spark.sql.files.maxPartitionBytes
    description: >-
      The maximum number of bytes to pack into a single partition when reading files.
      This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.
    enabled: true
    level: job
    category: tuning
    defaultMemoryUnit: Byte
    confType:
      name: byte
      defaultUnit: Byte
  - label: spark.sql.shuffle.partitions
    description: >-
      The default number of partitions to use when shuffling data for joins or aggregations.
      Note that for structured streaming, this configuration cannot be changed between query
      restarts from the same checkpoint location.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "200"
    confType:
      name: int
  - label: spark.task.resource.gpu.amount
    description: >-
      The GPU resource amount per task when Apache Spark schedules GPU resources.
      For example, setting the value to 1 means that only one task will run concurrently per executor.
      Note:
      - This value should be a fraction (e.g. 0.25, 1.0, etc.).
      - https://github.com/apache/spark/blob/6a7a3debda0748d7e72445de27ecc6ebfe01078b/core/src/main/scala/org/apache/spark/resource/TaskResourceRequest.scala#L37
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: double
  - label: spark.executor.resource.gpu.amount
    description: >-
      The GPU resource amount per executor when Spark schedules GPU resources.
      This should typically be set to 1 to allocate one GPU per executor.
      Note:
      - This value should be an integer (e.g. 1, 2, etc.).
      - https://github.com/apache/spark/blob/6a7a3debda0748d7e72445de27ecc6ebfe01078b/core/src/main/scala/org/apache/spark/resource/ExecutorResourceRequest.scala#L56
    enabled: true
    level: cluster
    category: functionality
    confType:
      name: long
    comments:
      missing: should be set to allow Spark to schedule GPU resources.
  - label: spark.executor.resource.gpu.discoveryScript
    description: >-
      The absolute path to a discovery script that Spark will run on each executor to determine available GPU resources.
      This is needed for YARN and K8s clusters.
      Defaults to '${SPARK_HOME}/examples/src/main/scripts/getGpusResources.sh'
      Reference:
       - https://docs.nvidia.com/spark-rapids/user-guide/latest/getting-started/overview.html
       - https://github.com/apache/spark/blob/master/examples/src/main/scripts/getGpusResources.sh
    enabled: true
    level: cluster
    category: functionality
    confType:
      name: string
  - label: spark.executor.resource.gpu.vendor
    description: >-
      The vendor of the GPU resource. This is needed for k8s clusters.
      For NVIDIA GPUs, this should be set to "nvidia.com".
      Reference:
       - https://spark.apache.org/docs/latest/running-on-kubernetes.html
       - https://docs.nvidia.com/spark-rapids/user-guide/latest/getting-started/kubernetes.html
    enabled: true
    level: cluster
    category: functionality
    confType:
      name: string
  - label: spark.plugins
    description: >-
      A comma-separated list of plugin class names to be loaded by Spark.
      Setting this to "com.nvidia.spark.SQLPlugin" enables the RAPIDS Accelerator plugin.
    enabled: true
    level: cluster
    category: functionality
    confType:
      name: string
    comments:
      missing: |-
        should be set to the class name required for the RAPIDS Accelerator for Apache Spark.
          Refer to: https://docs.nvidia.com/spark-rapids/user-guide/latest/getting-started/overview.html
      updated: |-
        should include the class name required for the RAPIDS Accelerator for Apache Spark.
          Refer to: https://docs.nvidia.com/spark-rapids/user-guide/latest/getting-started/overview.html
  - label: spark.rapids.sql.incompatibleDateFormats.enabled
    description: >-
      When true, enables support for certain date formats that are incompatible with the GPU.
      It is not guaranteed to produce the same results as Spark on CPU.
    enabled: true
    level: job
    category: functionality
    confType:
      name: boolean
  - label: spark.dynamicAllocation.initialExecutors
    description: >-
      Initial number of executors to run if dynamic allocation is enabled.
      If --num-executors (or spark.executor.instances) is set and larger than this value, it will
      be used as the initial number of executors.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: int
  - label: spark.dynamicAllocation.minExecutors
    description: >-
      Lower bound for the number of executors if dynamic allocation is enabled.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: int
  - label: spark.dynamicAllocation.maxExecutors
    description: >-
      Upper bound for the number of executors if dynamic allocation is enabled.
    enabled: true
    level: cluster
    category: tuning
    confType:
      name: int

  ##############################################################################
  # Plugin Properties Entries
  ##############################################################################
  #-----------------------------------------------------------------------------
  # Delta Lake Related properties Rules
  #-----------------------------------------------------------------------------
  - label: spark.databricks.delta.deletionVectors.useMetadataRowIndex
    # see https://books.japila.pl/delta-lake-internals/configuration-properties/#deletionVectors.useMetadataRowIndex
    description: >-
      Enables using the Parquet reader generated "row_index" column for filtering deleted rows with
      Deletion Vectors (that gives predicate pushdown and file splitting in scans).
      This config should be disabled to use GPU acceleration for Delta Lake tables with Deletion Vectors.
      Otherwise, all scans will fall back to CPU.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "true"
    confType:
      name: boolean
    comments:
      persistent: >-
        should be disabled. WARN: Turning this property on causes scan against tables with deletion vectors to fallback to the CPU.
  # optimized writes for delta Lake
  - label: spark.databricks.delta.optimizeWrite.enabled
    description: >-
      Enable optimized writes for Delta Lake tables at the Spark session level rather that the table level. Optimized writes improve file size as data is written and benefit subsequent reads on the table.
      Note that this can be table property 'delta.autoOptimize.optimizeWrite'.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "false"
    modifiedBy: DeltaOSSWriteRule
    confType:
      name: boolean
    comments:
      updated: The optimized write feature on Delta Lake is disabled by default. It can be enabled on SQL session level to improve file size.
  - label: spark.databricks.delta.optimizeWrite.binSize
    description: >-
      Target uncompressed partition size in megabytes. This controls the target in-memory size of each output file.
      For details, Refer to Delta Lake docs https://docs.delta.io/optimizations-oss/#optimized-write.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "512MiB"
    confType:
      name: byte
      defaultUnit: MiB
  - label: spark.databricks.delta.optimizeWrite.smallPartitionFactor
    # TODO: maybe this should be specific to DB environments.
    description: >-
      Merge partitions smaller than this factor multiplied by the target partition size.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "0.5"
    confType:
      name: double
  - label: spark.databricks.delta.optimizeWrite.mergedPartitionFactor
    description: >-
      Avoid combining partitions larger than this factor multiplied by the target partition size
    enabled: true
    level: job
    category: tuning
    defaultSpark: "1.2"
    confType:
      name: double
  # automatic Compaction for Delta Lake OSS Delta
  - label: spark.databricks.delta.autoCompact.enabled
    description: >-
      Auto compaction combines small files within Delta table partitions to automatically reduce small file problems.
      Note that this can be table property 'delta.autoOptimize.autoCompact'.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "false"
    confType:
      name: boolean
    comments:
      updated: allows enabling auto compaction on Databricks Delta Lake at the Spark session level.
  - label: spark.databricks.delta.autoCompact.minNumFiles
    description: >-
      Minimum number of files that must exist in the Delta directory before Auto Optimize begins compaction.
      See Delta Lake docs https://docs.delta.io/optimizations-oss/#auto-compaction.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "50"
    confType:
      name: int
  - label: spark.databricks.delta.autoCompact.maxFileSize
    description: >-
      Target file size produced by auto compaction.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "128MiB"
    confType:
      name: byte
      defaultUnit: MiB
  - label: spark.databricks.delta.autoCompact.minFileSize
    description: >-
      Files which are smaller than this threshold (in bytes) will be grouped together and rewritten as larger files by the auto compaction.
      By default, it set to half of 'spark.databricks.delta.autoCompact.maxFileSize'
    enabled: true
    level: job
    category: tuning
    confType:
      name: byte
      defaultUnit: Byte
  # automatic Compaction for Databricks Delta Lake
  - label: spark.databricks.delta.properties.defaults.autoOptimize.autoCompact
    description: >-
      Whether to enable auto compaction by default, if 'spark.databricks.delta.autoCompact.enabled' isn't set.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "false"
    confType:
      name: boolean
    comments:
      missing: The feature on Delta Lake is disabled by default.
  - label: spark.databricks.delta.autoCompact.minNumFiles
    description: >-
      Minimum number of files that must exist in the Delta directory before Auto Optimize begins compaction.
      See Delta Lake docs https://docs.delta.io/optimizations-oss/#auto-compaction.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "50"
    confType:
      name: int
  - label: spark.databricks.delta.autoCompact.maxFileSize
    description: >-
      Target file size produced by auto compaction.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "128MiB"
    confType:
      name: byte
      defaultUnit: MiB
  - label: spark.databricks.delta.autoCompact.target
    description: >-
      Target files for auto compaction. "table", "commit", "partition" options are available. If "table", all files in the table are eligible for auto compaction. If "commit", added/updated files by the commit are eligible. If "partition", all files in partitions containing any added/updated files by the commit are eligible.
    enabled: true
    level: job
    category: tuning
    defaultSpark: partition
    confType:
      name: string
  - label: spark.databricks.delta.autoCompact.maxCompactBytes
    description: >-
      Maximum amount of data to compact together.
    enabled: true
    level: job
    category: tuning
    defaultSpark: 20GB
    confType:
      name: byte
      defaultUnit: Byte
  # delta update commands
  - label: spark.rapids.sql.command.MergeIntoCommand
    description: >-
      Acceleration is enabled by default. Set this to False to disable GPU acceleration for Delta Lake Merge Into commands.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "true"
    confType:
      name: boolean
    comments:
      persistent: controls whether the CMD is accelerated by the Spark RAPIDS Accelerator. Default is "true".
  - label: spark.rapids.sql.command.DeleteCommand
    description: >-
      Acceleration is enabled by default. Set this to False to disable GPU acceleration for Delta Lake Delete commands.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "true"
    confType:
      name: boolean
    comments:
      persistent: controls whether the CMD is accelerated by the Spark RAPIDS Accelerator. Default is "true".
  - label: spark.rapids.sql.command.UpdateCommand
    description: >-
      Acceleration is enabled by default. Set this to False to disable GPU acceleration for Delta Lake Update commands.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "true"
    confType:
      name: boolean
    comments:
      persistent: controls whether the CMD is accelerated by the Spark RAPIDS Accelerator. Default is "true".
  # Databricks delta update commands
  - label: spark.rapids.sql.command.MergeIntoCommandEdge
    description: >-
      Acceleration is enabled by default. Set this to False to disable GPU acceleration for Delta Lake Merge Into commands.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "true"
    confType:
      name: boolean
    comments:
      persistent: controls whether the CMD is accelerated by the Spark RAPIDS Accelerator. Default is "true".
  - label: spark.rapids.sql.command.DeleteCommandEdge
    description: >-
      Acceleration is enabled by default. Set this to False to disable GPU acceleration for Delta Lake Delete commands.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "true"
    confType:
      name: boolean
    comments:
      persistent: controls whether the CMD is accelerated by the Spark RAPIDS Accelerator. Default is "true".
  - label: spark.rapids.sql.command.UpdateCommandEdge
    description: >-
      Acceleration is enabled by default. Set this to False to disable GPU acceleration for Delta Lake Update commands.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "true"
    confType:
      name: boolean
    comments:
      persistent: controls whether the CMD is accelerated by the Spark RAPIDS Accelerator. Default is "true".
  - label: spark.rapids.sql.format.delta.write.enabled
    description: >-
      Acceleration is enabled by default. Set this to False to disable GPU acceleration for Delta Lake Update commands.
    enabled: true
    level: job
    category: tuning
    defaultSpark: "true"
    confType:
      name: boolean
    comments:
      persistent: controls whether the writing of Delta Lake tables is accelerated by the Spark RAPIDS Accelerator. Default is 'true'.
