{"Event":"SparkListenerLogStart","Spark Version":"3.1.1"}
{"Event":"SparkListenerResourceProfileAdded","Resource Profile Id":0,"Executor Resource Requests":{"cores":{"Resource Name":"cores","Amount":1,"Discovery Script":"","Vendor":""},"memory":{"Resource Name":"memory","Amount":1024,"Discovery Script":"","Vendor":""},"offHeap":{"Resource Name":"offHeap","Amount":0,"Discovery Script":"","Vendor":""}},"Task Resource Requests":{"cpus":{"Resource Name":"cpus","Amount":1.0}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"driver","Host":"user1.attlocal.net","Port":38991},"Maximum Memory":384093388,"Timestamp":1651187225711,"Maximum Onheap Memory":384093388,"Maximum Offheap Memory":0}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/usr/lib/jvm/zulu8-ca-arm64/jre","Java Version":"1.8.0_362 (Azul Systems,Inc.)","Scala Version":"version 2.12.14"},"Spark Properties":{"spark.databricks.workerNodeTypeId":"m6gd.2xlarge","spark.databricks.driverNodeTypeId":"m6gd.2xlarge"},"Hadoop Properties":{"hadoop.service.shutdown.timeout":"30s","fs.s3a.connection.maximum":"200"},"System Properties":{"java.io.tmpdir":"/local_disk0/tmp"},"Classpath Entries":{"/databricks/jars/----ws_3_3--core--core-hive-2.3__hadoop-3.2_2.12_deploy.jar":"System Classpath"},"Build Properties":{"Runtime Build Hash":"4c66f5dbce4855d0c379a270bd19d2bc0ed066c1","Universe Build Hash":"872f906f90c723e237bc469854cbc41c3be33d3d"}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1702417715262,"Executor ID":"1","Executor Info":{"Host":"10.59.184.228","Total Cores":12,"Log Urls":{"stdout":"http://10.59.184.228:40000/logPage/?appId=app-20231212214826-0000&executorId=1&logType=stdout","stderr":"http://10.59.184.228:40000/logPage/?appId=app-20231212214826-0000&executorId=1&logType=stderr"},"Attributes":{},"Resources":{},"Resource Profile Id":0}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"10.59.184.228","Port":46767},"Maximum Memory":11884245811,"Timestamp":1702417716594,"Maximum Onheap Memory":11884245811,"Maximum Offheap Memory":0}
{"Event":"SparkListenerExecutorAdded","Timestamp":1702417716738,"Executor ID":"0","Executor Info":{"Host":"10.59.184.244","Total Cores":12,"Log Urls":{"stdout":"http://10.59.184.244:40000/logPage/?appId=app-20231212214826-0000&executorId=0&logType=stdout","stderr":"http://10.59.184.244:40000/logPage/?appId=app-20231212214826-0000&executorId=0&logType=stderr"},"Attributes":{},"Resources":{},"Resource Profile Id":0}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"10.59.184.244","Port":43349},"Maximum Memory":11884245811,"Timestamp":1702417718124,"Maximum Onheap Memory":11884245811,"Maximum Offheap Memory":0}
{"Event":"SparkListenerExecutorAdded","Timestamp":1702417715262,"Executor ID":"2","Executor Info":{"Host":"10.59.184.256","Total Cores":12,"Log Urls":{"stdout":"http://10.59.184.228:40000/logPage/?appId=app-20231212214826-0000&executorId=1&logType=stdout","stderr":"http://10.59.184.228:40000/logPage/?appId=app-20231212214826-0000&executorId=1&logType=stderr"},"Attributes":{},"Resources":{},"Resource Profile Id":0}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"2","Host":"10.59.184.256","Port":46767},"Maximum Memory":11884245811,"Timestamp":1702417716594,"Maximum Onheap Memory":11884245811,"Maximum Offheap Memory":0}
{"Event":"SparkListenerExecutorAdded","Timestamp":1702417716738,"Executor ID":"3","Executor Info":{"Host":"10.59.184.244","Total Cores":12,"Log Urls":{"stdout":"http://10.59.184.244:40000/logPage/?appId=app-20231212214826-0000&executorId=0&logType=stdout","stderr":"http://10.59.184.244:40000/logPage/?appId=app-20231212214826-0000&executorId=0&logType=stderr"},"Attributes":{},"Resources":{},"Resource Profile Id":0}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"3","Host":"10.59.184.244","Port":43349},"Maximum Memory":11884245811,"Timestamp":1702417718124,"Maximum Onheap Memory":11884245811,"Maximum Offheap Memory":0}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/usr/lib/jvm/zulu8-ca-arm64/jre","Java Version":"1.8.0_362 (Azul Systems,Inc.)","Scala Version":"version 2.12.14"},"Spark Properties":{"spark.databricks.workerNodeTypeId":"m6gd.2xlarge","spark.databricks.driverNodeTypeId":"m6gd.2xlarge"},"Hadoop Properties":{"hadoop.service.shutdown.timeout":"30s","fs.s3a.connection.maximum":"200"},"System Properties":{"java.io.tmpdir":"/local_disk0/tmp"},"Classpath Entries":{"/databricks/jars/----ws_3_3--core--core-hive-2.3__hadoop-3.2_2.12_deploy.jar":"System Classpath"},"Build Properties":{"Runtime Build Hash":"4c66f5dbce4855d0c379a270bd19d2bc0ed066c1","Universe Build Hash":"872f906f90c723e237bc469854cbc41c3be33d3d"}}
{"Event":"SparkListenerApplicationStart","App Name":"Spark shell","App ID":"local-1726394182053-db","Timestamp":1651187224195,"User":"user1"}
{"Event":"SparkListenerJobStart","Job ID":0,"Submission Time":1651187428498,"Stage Infos":[{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"json at <console>:25","Number of Tasks":1,"RDD Info":[{"RDD ID":3,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"mapPartitions\"}","Callsite":"json at <console>:25","Parent IDs":[2],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"FileScanRDD","Scope":"{\"id\":\"0\",\"name\":\"Scan text \"}","Callsite":"json at <console>:25","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":2,"Name":"SQLExecutionRDD","Callsite":"json at <console>:25","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"0\",\"name\":\"Scan text \"}","Callsite":"json at <console>:25","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:429)\n$line15.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:25)\n$line15.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)\n$line15.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)\n$line15.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:33)\n$line15.$read$$iw$$iw$$iw$$iw.<init>(<console>:35)\n$line15.$read$$iw$$iw$$iw.<init>(<console>:37)\n$line15.$read$$iw$$iw.<init>(<console>:39)\n$line15.$read$$iw.<init>(<console>:41)\n$line15.$read.<init>(<console>:43)\n$line15.$read$.<init>(<console>:47)\n$line15.$read$.<clinit>(<console>)\n$line15.$eval$.$print$lzycompute(<console>:7)\n$line15.$eval$.$print(<console>:6)\n$line15.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nscala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)","Accumulables":[],"Resource Profile Id":0}],"Stage IDs":[0],"Properties":{"spark.sql.warehouse.dir":"file:/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2/spark-warehouse/","spark.driver.host":"user1.attlocal.net","spark.eventLog.enabled":"true","spark.driver.port":"41345","spark.repl.class.uri":"spark://user1.attlocal.net:41345/classes","spark.jars":"","spark.repl.class.outputDir":"/tmp/spark-7f3a4b49-f48c-4e53-8ea3-a16861c74338/repl-01d6fffe-ba55-49e7-afdb-bbf08996d7f1","spark.app.name":"Spark shell","spark.submit.pyFiles":"","spark.ui.showConsoleProgress":"true","spark.app.startTime":"1651187224195","spark.executor.id":"driver","spark.submit.deployMode":"client","spark.master":"local[*]","spark.home":"/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2","spark.eventLog.dir":"/home/user1/new_eventlogs","spark.sql.catalogImplementation":"hive","spark.app.id":"local-1651187225439"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"json at <console>:25","Number of Tasks":1,"RDD Info":[{"RDD ID":3,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"mapPartitions\"}","Callsite":"json at <console>:25","Parent IDs":[2],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"FileScanRDD","Scope":"{\"id\":\"0\",\"name\":\"Scan text \"}","Callsite":"json at <console>:25","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":2,"Name":"SQLExecutionRDD","Callsite":"json at <console>:25","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"0\",\"name\":\"Scan text \"}","Callsite":"json at <console>:25","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:429)\n$line15.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:25)\n$line15.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)\n$line15.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)\n$line15.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:33)\n$line15.$read$$iw$$iw$$iw$$iw.<init>(<console>:35)\n$line15.$read$$iw$$iw$$iw.<init>(<console>:37)\n$line15.$read$$iw$$iw.<init>(<console>:39)\n$line15.$read$$iw.<init>(<console>:41)\n$line15.$read.<init>(<console>:43)\n$line15.$read$.<init>(<console>:47)\n$line15.$read$.<clinit>(<console>)\n$line15.$eval$.$print$lzycompute(<console>:7)\n$line15.$eval$.$print(<console>:6)\n$line15.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nscala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)","Submission Time":1651187428532,"Accumulables":[],"Resource Profile Id":0},"Properties":{"spark.sql.warehouse.dir":"file:/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2/spark-warehouse/","spark.driver.host":"user1.attlocal.net","spark.eventLog.enabled":"true","spark.driver.port":"41345","spark.repl.class.uri":"spark://user1.attlocal.net:41345/classes","spark.jars":"","spark.repl.class.outputDir":"/tmp/spark-7f3a4b49-f48c-4e53-8ea3-a16861c74338/repl-01d6fffe-ba55-49e7-afdb-bbf08996d7f1","spark.app.name":"Spark shell","spark.submit.pyFiles":"","spark.ui.showConsoleProgress":"true","spark.app.startTime":"1651187224195","spark.executor.id":"driver","spark.submit.deployMode":"client","spark.master":"local[*]","spark.home":"/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2","spark.eventLog.dir":"/home/user1/new_eventlogs","spark.sql.catalogImplementation":"hive","spark.app.id":"local-1651187225439"}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1651187428777,"Executor ID":"driver","Host":"user1.attlocal.net","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1651187428777,"Executor ID":"driver","Host":"user1.attlocal.net","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1651187430025,"Failed":false,"Killed":false,"Accumulables":[{"ID":0,"Name":"number of output rows","Update":"3","Value":"3","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":4,"Name":"internal.metrics.executorDeserializeTime","Update":484,"Value":484,"Internal":true,"Count Failed Values":true},{"ID":5,"Name":"internal.metrics.executorDeserializeCpuTime","Update":453090462,"Value":453090462,"Internal":true,"Count Failed Values":true},{"ID":6,"Name":"internal.metrics.executorRunTime","Update":707,"Value":707,"Internal":true,"Count Failed Values":true},{"ID":7,"Name":"internal.metrics.executorCpuTime","Update":602761120,"Value":602761120,"Internal":true,"Count Failed Values":true},{"ID":8,"Name":"internal.metrics.resultSize","Update":2080,"Value":2080,"Internal":true,"Count Failed Values":true},{"ID":9,"Name":"internal.metrics.jvmGCTime","Update":18,"Value":18,"Internal":true,"Count Failed Values":true},{"ID":10,"Name":"internal.metrics.resultSerializationTime","Update":1,"Value":1,"Internal":true,"Count Failed Values":true},{"ID":25,"Name":"internal.metrics.input.bytesRead","Update":133,"Value":133,"Internal":true,"Count Failed Values":true},{"ID":26,"Name":"internal.metrics.input.recordsRead","Update":3,"Value":3,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0},"Task Metrics":{"Executor Deserialize Time":484,"Executor Deserialize CPU Time":453090462,"Executor Run Time":707,"Executor CPU Time":602761120,"Peak Execution Memory":0,"Result Size":2080,"JVM GC Time":18,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":133,"Records Read":3},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"json at <console>:25","Number of Tasks":1,"RDD Info":[{"RDD ID":3,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"mapPartitions\"}","Callsite":"json at <console>:25","Parent IDs":[2],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"FileScanRDD","Scope":"{\"id\":\"0\",\"name\":\"Scan text \"}","Callsite":"json at <console>:25","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":2,"Name":"SQLExecutionRDD","Callsite":"json at <console>:25","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"0\",\"name\":\"Scan text \"}","Callsite":"json at <console>:25","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:429)\n$line15.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:25)\n$line15.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)\n$line15.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)\n$line15.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:33)\n$line15.$read$$iw$$iw$$iw$$iw.<init>(<console>:35)\n$line15.$read$$iw$$iw$$iw.<init>(<console>:37)\n$line15.$read$$iw$$iw.<init>(<console>:39)\n$line15.$read$$iw.<init>(<console>:41)\n$line15.$read.<init>(<console>:43)\n$line15.$read$.<init>(<console>:47)\n$line15.$read$.<clinit>(<console>)\n$line15.$eval$.$print$lzycompute(<console>:7)\n$line15.$eval$.$print(<console>:6)\n$line15.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nscala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)","Submission Time":1651187428532,"Completion Time":1651187430036,"Accumulables":[{"ID":0,"Name":"number of output rows","Value":"3","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":4,"Name":"internal.metrics.executorDeserializeTime","Value":484,"Internal":true,"Count Failed Values":true},{"ID":5,"Name":"internal.metrics.executorDeserializeCpuTime","Value":453090462,"Internal":true,"Count Failed Values":true},{"ID":6,"Name":"internal.metrics.executorRunTime","Value":707,"Internal":true,"Count Failed Values":true},{"ID":7,"Name":"internal.metrics.executorCpuTime","Value":602761120,"Internal":true,"Count Failed Values":true},{"ID":8,"Name":"internal.metrics.resultSize","Value":2080,"Internal":true,"Count Failed Values":true},{"ID":9,"Name":"internal.metrics.jvmGCTime","Value":18,"Internal":true,"Count Failed Values":true},{"ID":10,"Name":"internal.metrics.resultSerializationTime","Value":1,"Internal":true,"Count Failed Values":true},{"ID":25,"Name":"internal.metrics.input.bytesRead","Value":133,"Internal":true,"Count Failed Values":true},{"ID":26,"Name":"internal.metrics.input.recordsRead","Value":3,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0}}
{"Event":"SparkListenerJobEnd","Job ID":0,"Completion Time":1651187430050,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":0,"description":"show at <console>:26","details":"org.apache.spark.sql.Dataset.show(Dataset.scala:793)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:26)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:30)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:32)\n$line21.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:34)\n$line21.$read$$iw$$iw$$iw$$iw.<init>(<console>:36)\n$line21.$read$$iw$$iw$$iw.<init>(<console>:38)\n$line21.$read$$iw$$iw.<init>(<console>:40)\n$line21.$read$$iw.<init>(<console>:42)\n$line21.$read.<init>(<console>:44)\n$line21.$read$.<init>(<console>:48)\n$line21.$read$.<clinit>(<console>)\n$line21.$eval$.$print$lzycompute(<console>:7)\n$line21.$eval$.$print(<console>:6)\n$line21.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nscala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (3)\n+- * Project (2)\n   +- Scan json  (1)\n\n\n(1) Scan json \nOutput [3]: [age#7L, lname#8, name#9]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/user1/json-reader/people.json]\nReadSchema: struct<age:bigint,lname:string,name:string>\n\n(2) Project [codegen id : 1]\nOutput [4]: [cast(age#7L as string) AS age#30, lname#8, name#9, cast(age#7L as string) AS ageCategory#33]\nInput [3]: [age#7L, lname#8, name#9]\n\n(3) CollectLimit\nInput [4]: [age#30, lname#8, name#9, ageCategory#33]\nArguments: 21\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 21","children":[{"nodeName":"WholeStageCodegen (1)","simpleString":"WholeStageCodegen (1)","children":[{"nodeName":"Project","simpleString":"Project [cast(age#7L as string) AS age#30, lname#8, name#9, cast(age#7L as string) AS ageCategory#33]","children":[{"nodeName":"InputAdapter","simpleString":"InputAdapter","children":[{"nodeName":"Scan json ","simpleString":"FileScan json [age#7L,lname#8,name#9] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex[file:/home/user1/json-reader/people.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint,lname:string,name:string>","children":[],"metadata":{"Location":"InMemoryFileIndex[file:/home/user1/json-reader/people.json]","ReadSchema":"struct<age:bigint,lname:string,name:string>","Format":"JSON","Batched":"false","PartitionFilters":"[]","PushedFilters":"[]","DataFilters":"[]"},"metrics":[{"name":"number of output rows","accumulatorId":40,"metricType":"sum"},{"name":"number of files read","accumulatorId":41,"metricType":"sum"},{"name":"metadata time","accumulatorId":42,"metricType":"timing"},{"name":"size of files read","accumulatorId":43,"metricType":"size"}]}],"metadata":{},"metrics":[]}],"metadata":{},"metrics":[]}],"metadata":{},"metrics":[{"name":"duration","accumulatorId":39,"metricType":"timing"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":37,"metricType":"sum"},{"name":"shuffle write time","accumulatorId":38,"metricType":"nsTiming"},{"name":"records read","accumulatorId":35,"metricType":"sum"},{"name":"local bytes read","accumulatorId":33,"metricType":"size"},{"name":"fetch wait time","accumulatorId":34,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":31,"metricType":"size"},{"name":"local blocks read","accumulatorId":30,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":29,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":32,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":36,"metricType":"size"}]},"time":1651187510358}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates","executionId":0,"accumUpdates":[[41,1],[42,0],[43,133]]}
{"Event":"SparkListenerJobStart","Job ID":1,"Submission Time":1651187510580,"Stage Infos":[{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"show at <console>:26","Number of Tasks":1,"RDD Info":[{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"mapPartitionsInternal\"}","Callsite":"show at <console>:26","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":5,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"show at <console>:26","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":4,"Name":"FileScanRDD","Scope":"{\"id\":\"9\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.sql.Dataset.show(Dataset.scala:793)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:26)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:30)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:32)\n$line21.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:34)\n$line21.$read$$iw$$iw$$iw$$iw.<init>(<console>:36)\n$line21.$read$$iw$$iw$$iw.<init>(<console>:38)\n$line21.$read$$iw$$iw.<init>(<console>:40)\n$line21.$read$$iw.<init>(<console>:42)\n$line21.$read.<init>(<console>:44)\n$line21.$read$.<init>(<console>:48)\n$line21.$read$.<clinit>(<console>)\n$line21.$eval$.$print$lzycompute(<console>:7)\n$line21.$eval$.$print(<console>:6)\n$line21.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nscala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)","Accumulables":[],"Resource Profile Id":0}],"Stage IDs":[1],"Properties":{"spark.sql.warehouse.dir":"file:/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2/spark-warehouse/","spark.driver.host":"user1.attlocal.net","spark.eventLog.enabled":"true","spark.driver.port":"41345","spark.repl.class.uri":"spark://user1.attlocal.net:41345/classes","spark.jars":"","spark.repl.class.outputDir":"/tmp/spark-7f3a4b49-f48c-4e53-8ea3-a16861c74338/repl-01d6fffe-ba55-49e7-afdb-bbf08996d7f1","spark.app.name":"Spark shell","spark.submit.pyFiles":"","spark.ui.showConsoleProgress":"true","spark.app.startTime":"1651187224195","spark.executor.id":"driver","spark.submit.deployMode":"client","spark.master":"local[*]","spark.home":"/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2","spark.eventLog.dir":"/home/user1/new_eventlogs","spark.sql.execution.id":"0","spark.sql.catalogImplementation":"hive","spark.app.id":"local-1651187225439"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"show at <console>:26","Number of Tasks":1,"RDD Info":[{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"mapPartitionsInternal\"}","Callsite":"show at <console>:26","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":5,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"show at <console>:26","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":4,"Name":"FileScanRDD","Scope":"{\"id\":\"9\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.sql.Dataset.show(Dataset.scala:793)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:26)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:30)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:32)\n$line21.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:34)\n$line21.$read$$iw$$iw$$iw$$iw.<init>(<console>:36)\n$line21.$read$$iw$$iw$$iw.<init>(<console>:38)\n$line21.$read$$iw$$iw.<init>(<console>:40)\n$line21.$read$$iw.<init>(<console>:42)\n$line21.$read.<init>(<console>:44)\n$line21.$read$.<init>(<console>:48)\n$line21.$read$.<clinit>(<console>)\n$line21.$eval$.$print$lzycompute(<console>:7)\n$line21.$eval$.$print(<console>:6)\n$line21.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nscala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)","Submission Time":1651187510582,"Accumulables":[],"Resource Profile Id":0},"Properties":{"spark.sql.warehouse.dir":"file:/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2/spark-warehouse/","spark.driver.host":"user1.attlocal.net","spark.eventLog.enabled":"true","spark.driver.port":"41345","spark.repl.class.uri":"spark://user1.attlocal.net:41345/classes","spark.jars":"","spark.repl.class.outputDir":"/tmp/spark-7f3a4b49-f48c-4e53-8ea3-a16861c74338/repl-01d6fffe-ba55-49e7-afdb-bbf08996d7f1","spark.app.name":"Spark shell","spark.submit.pyFiles":"","spark.ui.showConsoleProgress":"true","spark.app.startTime":"1651187224195","spark.executor.id":"driver","spark.submit.deployMode":"client","spark.master":"local[*]","spark.home":"/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2","spark.eventLog.dir":"/home/user1/new_eventlogs","spark.sql.execution.id":"0","spark.sql.catalogImplementation":"hive","spark.app.id":"local-1651187225439"}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":1,"Index":0,"Attempt":0,"Launch Time":1651187510616,"Executor ID":"driver","Host":"user1.attlocal.net","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":1,"Index":0,"Attempt":0,"Launch Time":1651187510616,"Executor ID":"driver","Host":"user1.attlocal.net","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1651187510770,"Failed":false,"Killed":false,"Accumulables":[{"ID":39,"Name":"duration","Update":"74","Value":"74","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":40,"Name":"number of output rows","Update":"3","Value":"3","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":44,"Name":"internal.metrics.executorDeserializeTime","Update":54,"Value":54,"Internal":true,"Count Failed Values":true},{"ID":45,"Name":"internal.metrics.executorDeserializeCpuTime","Update":34471053,"Value":34471053,"Internal":true,"Count Failed Values":true},{"ID":46,"Name":"internal.metrics.executorRunTime","Update":87,"Value":87,"Internal":true,"Count Failed Values":true},{"ID":47,"Name":"internal.metrics.executorCpuTime","Update":75371480,"Value":75371480,"Internal":true,"Count Failed Values":true},{"ID":48,"Name":"internal.metrics.resultSize","Update":1574,"Value":1574,"Internal":true,"Count Failed Values":true},{"ID":65,"Name":"internal.metrics.input.bytesRead","Update":133,"Value":133,"Internal":true,"Count Failed Values":true},{"ID":66,"Name":"internal.metrics.input.recordsRead","Update":3,"Value":3,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0},"Task Metrics":{"Executor Deserialize Time":54,"Executor Deserialize CPU Time":34471053,"Executor Run Time":87,"Executor CPU Time":75371480,"Peak Execution Memory":0,"Result Size":1574,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":133,"Records Read":3},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"show at <console>:26","Number of Tasks":1,"RDD Info":[{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"mapPartitionsInternal\"}","Callsite":"show at <console>:26","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":5,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"show at <console>:26","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":4,"Name":"FileScanRDD","Scope":"{\"id\":\"9\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.sql.Dataset.show(Dataset.scala:793)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:26)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:30)\n$line21.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:32)\n$line21.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:34)\n$line21.$read$$iw$$iw$$iw$$iw.<init>(<console>:36)\n$line21.$read$$iw$$iw$$iw.<init>(<console>:38)\n$line21.$read$$iw$$iw.<init>(<console>:40)\n$line21.$read$$iw.<init>(<console>:42)\n$line21.$read.<init>(<console>:44)\n$line21.$read$.<init>(<console>:48)\n$line21.$read$.<clinit>(<console>)\n$line21.$eval$.$print$lzycompute(<console>:7)\n$line21.$eval$.$print(<console>:6)\n$line21.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nscala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)","Submission Time":1651187510582,"Completion Time":1651187510771,"Accumulables":[{"ID":39,"Name":"duration","Value":"74","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":40,"Name":"number of output rows","Value":"3","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":44,"Name":"internal.metrics.executorDeserializeTime","Value":54,"Internal":true,"Count Failed Values":true},{"ID":45,"Name":"internal.metrics.executorDeserializeCpuTime","Value":34471053,"Internal":true,"Count Failed Values":true},{"ID":46,"Name":"internal.metrics.executorRunTime","Value":87,"Internal":true,"Count Failed Values":true},{"ID":47,"Name":"internal.metrics.executorCpuTime","Value":75371480,"Internal":true,"Count Failed Values":true},{"ID":48,"Name":"internal.metrics.resultSize","Value":1574,"Internal":true,"Count Failed Values":true},{"ID":65,"Name":"internal.metrics.input.bytesRead","Value":133,"Internal":true,"Count Failed Values":true},{"ID":66,"Name":"internal.metrics.input.recordsRead","Value":3,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0}}
{"Event":"SparkListenerJobEnd","Job ID":1,"Completion Time":1651187510771,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":0,"time":1651187510856}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":1,"description":"show at <console>:26","details":"org.apache.spark.sql.Dataset.show(Dataset.scala:793)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:26)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:30)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:32)\n$line26.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:34)\n$line26.$read$$iw$$iw$$iw$$iw.<init>(<console>:36)\n$line26.$read$$iw$$iw$$iw.<init>(<console>:38)\n$line26.$read$$iw$$iw.<init>(<console>:40)\n$line26.$read$$iw.<init>(<console>:42)\n$line26.$read.<init>(<console>:44)\n$line26.$read$.<init>(<console>:48)\n$line26.$read$.<clinit>(<console>)\n$line26.$eval$.$print$lzycompute(<console>:7)\n$line26.$eval$.$print(<console>:6)\n$line26.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nscala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (7)\n+- * Project (6)\n   +- * SerializeFromObject (5)\n      +- * MapElements (4)\n         +- * Filter (3)\n            +- * DeserializeToObject (2)\n               +- Scan json  (1)\n\n\n(1) Scan json \nOutput [3]: [age#7L, lname#8, name#9]\nBatched: false\nLocation: InMemoryFileIndex [file:/home/user1/json-reader/people.json]\nReadSchema: struct<age:bigint,lname:string,name:string>\n\n(2) DeserializeToObject [codegen id : 1]\nInput [3]: [age#7L, lname#8, name#9]\nArguments: newInstance(class $line14.$read$$iw$$iw$Person), obj#52: $line14.$read$$iw$$iw$Person\n\n(3) Filter [codegen id : 1]\nInput [1]: [obj#52]\nCondition : $line24.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3918/114354150@419ffde3.apply\n\n(4) MapElements [codegen id : 1]\nInput [1]: [obj#52]\nArguments: $line24.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3919/171888944@5bbcd407, obj#53: scala.Tuple2\n\n(5) SerializeFromObject [codegen id : 1]\nInput [1]: [obj#53]\nArguments: [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false) AS _1#54, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2 AS _2#55L]\n\n(6) Project [codegen id : 1]\nOutput [2]: [_1#54, cast(_2#55L as string) AS _2#62]\nInput [2]: [_1#54, _2#55L]\n\n(7) CollectLimit\nInput [2]: [_1#54, _2#62]\nArguments: 21\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 21","children":[{"nodeName":"WholeStageCodegen (1)","simpleString":"WholeStageCodegen (1)","children":[{"nodeName":"Project","simpleString":"Project [_1#54, cast(_2#55L as string) AS _2#62]","children":[{"nodeName":"SerializeFromObject","simpleString":"SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false) AS _1#54, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2 AS _2#55L]","children":[{"nodeName":"MapElements","simpleString":"MapElements $line24.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3919/171888944@5bbcd407, obj#53: scala.Tuple2","children":[{"nodeName":"Filter","simpleString":"Filter $line24.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3918/114354150@419ffde3.apply","children":[{"nodeName":"DeserializeToObject","simpleString":"DeserializeToObject newInstance(class $line14.$read$$iw$$iw$Person), obj#52: $line14.$read$$iw$$iw$Person","children":[{"nodeName":"InputAdapter","simpleString":"InputAdapter","children":[{"nodeName":"Scan json ","simpleString":"FileScan json [age#7L,lname#8,name#9] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex[file:/home/user1/json-reader/people.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint,lname:string,name:string>","children":[],"metadata":{"Location":"InMemoryFileIndex[file:/home/user1/json-reader/people.json]","ReadSchema":"struct<age:bigint,lname:string,name:string>","Format":"JSON","Batched":"false","PartitionFilters":"[]","PushedFilters":"[]","DataFilters":"[]"},"metrics":[{"name":"number of output rows","accumulatorId":81,"metricType":"sum"},{"name":"number of files read","accumulatorId":82,"metricType":"sum"},{"name":"metadata time","accumulatorId":83,"metricType":"timing"},{"name":"size of files read","accumulatorId":84,"metricType":"size"}]}],"metadata":{},"metrics":[]}],"metadata":{},"metrics":[]}],"metadata":{},"metrics":[{"name":"number of output rows","accumulatorId":80,"metricType":"sum"}]}],"metadata":{},"metrics":[]}],"metadata":{},"metrics":[]}],"metadata":{},"metrics":[]}],"metadata":{},"metrics":[{"name":"duration","accumulatorId":79,"metricType":"timing"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":77,"metricType":"sum"},{"name":"shuffle write time","accumulatorId":78,"metricType":"nsTiming"},{"name":"records read","accumulatorId":75,"metricType":"sum"},{"name":"local bytes read","accumulatorId":73,"metricType":"size"},{"name":"fetch wait time","accumulatorId":74,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":71,"metricType":"size"},{"name":"local blocks read","accumulatorId":70,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":69,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":72,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":76,"metricType":"size"}]},"time":1651187567912}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates","executionId":1,"accumUpdates":[[82,1],[83,0],[84,133]]}
{"Event":"SparkListenerJobStart","Job ID":2,"Submission Time":1651187568101,"Stage Infos":[{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"show at <console>:26","Number of Tasks":1,"RDD Info":[{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"23\",\"name\":\"mapPartitionsInternal\"}","Callsite":"show at <console>:26","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"22\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":8,"Name":"FileScanRDD","Scope":"{\"id\":\"22\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"15\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"show at <console>:26","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.sql.Dataset.show(Dataset.scala:793)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:26)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:30)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:32)\n$line26.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:34)\n$line26.$read$$iw$$iw$$iw$$iw.<init>(<console>:36)\n$line26.$read$$iw$$iw$$iw.<init>(<console>:38)\n$line26.$read$$iw$$iw.<init>(<console>:40)\n$line26.$read$$iw.<init>(<console>:42)\n$line26.$read.<init>(<console>:44)\n$line26.$read$.<init>(<console>:48)\n$line26.$read$.<clinit>(<console>)\n$line26.$eval$.$print$lzycompute(<console>:7)\n$line26.$eval$.$print(<console>:6)\n$line26.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nscala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)","Accumulables":[],"Resource Profile Id":0}],"Stage IDs":[2],"Properties":{"spark.sql.warehouse.dir":"file:/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2/spark-warehouse/","spark.driver.host":"user1.attlocal.net","spark.eventLog.enabled":"true","spark.driver.port":"41345","spark.repl.class.uri":"spark://user1.attlocal.net:41345/classes","spark.jars":"","spark.repl.class.outputDir":"/tmp/spark-7f3a4b49-f48c-4e53-8ea3-a16861c74338/repl-01d6fffe-ba55-49e7-afdb-bbf08996d7f1","spark.app.name":"Spark shell","spark.submit.pyFiles":"","spark.ui.showConsoleProgress":"true","spark.app.startTime":"1651187224195","spark.executor.id":"driver","spark.submit.deployMode":"client","spark.master":"local[*]","spark.home":"/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2","spark.eventLog.dir":"/home/user1/new_eventlogs","spark.sql.execution.id":"1","spark.sql.catalogImplementation":"hive","spark.app.id":"local-1651187225439"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"show at <console>:26","Number of Tasks":1,"RDD Info":[{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"23\",\"name\":\"mapPartitionsInternal\"}","Callsite":"show at <console>:26","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"22\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":8,"Name":"FileScanRDD","Scope":"{\"id\":\"22\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"15\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"show at <console>:26","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.sql.Dataset.show(Dataset.scala:793)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:26)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:30)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:32)\n$line26.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:34)\n$line26.$read$$iw$$iw$$iw$$iw.<init>(<console>:36)\n$line26.$read$$iw$$iw$$iw.<init>(<console>:38)\n$line26.$read$$iw$$iw.<init>(<console>:40)\n$line26.$read$$iw.<init>(<console>:42)\n$line26.$read.<init>(<console>:44)\n$line26.$read$.<init>(<console>:48)\n$line26.$read$.<clinit>(<console>)\n$line26.$eval$.$print$lzycompute(<console>:7)\n$line26.$eval$.$print(<console>:6)\n$line26.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nscala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)","Submission Time":1651187568103,"Accumulables":[],"Resource Profile Id":0},"Properties":{"spark.sql.warehouse.dir":"file:/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2/spark-warehouse/","spark.driver.host":"user1.attlocal.net","spark.eventLog.enabled":"true","spark.driver.port":"41345","spark.repl.class.uri":"spark://user1.attlocal.net:41345/classes","spark.jars":"","spark.repl.class.outputDir":"/tmp/spark-7f3a4b49-f48c-4e53-8ea3-a16861c74338/repl-01d6fffe-ba55-49e7-afdb-bbf08996d7f1","spark.app.name":"Spark shell","spark.submit.pyFiles":"","spark.ui.showConsoleProgress":"true","spark.app.startTime":"1651187224195","spark.executor.id":"driver","spark.submit.deployMode":"client","spark.master":"local[*]","spark.home":"/home/user1/spark-3.1.1/spark-3.1.1-bin-hadoop3.2","spark.eventLog.dir":"/home/user1/new_eventlogs","spark.sql.execution.id":"1","spark.sql.catalogImplementation":"hive","spark.app.id":"local-1651187225439"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":2,"Index":0,"Attempt":0,"Launch Time":1651187568115,"Executor ID":"driver","Host":"user1.attlocal.net","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":2,"Index":0,"Attempt":0,"Launch Time":1651187568115,"Executor ID":"driver","Host":"user1.attlocal.net","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1651187568141,"Failed":false,"Killed":false,"Accumulables":[{"ID":79,"Name":"duration","Update":"6","Value":"6","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":80,"Name":"number of output rows","Update":"2","Value":"2","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":81,"Name":"number of output rows","Update":"3","Value":"3","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":85,"Name":"internal.metrics.executorDeserializeTime","Update":10,"Value":10,"Internal":true,"Count Failed Values":true},{"ID":86,"Name":"internal.metrics.executorDeserializeCpuTime","Update":10148306,"Value":10148306,"Internal":true,"Count Failed Values":true},{"ID":87,"Name":"internal.metrics.executorRunTime","Update":12,"Value":12,"Internal":true,"Count Failed Values":true},{"ID":88,"Name":"internal.metrics.executorCpuTime","Update":12020701,"Value":12020701,"Internal":true,"Count Failed Values":true},{"ID":89,"Name":"internal.metrics.resultSize","Update":1561,"Value":1561,"Internal":true,"Count Failed Values":true},{"ID":106,"Name":"internal.metrics.input.bytesRead","Update":133,"Value":133,"Internal":true,"Count Failed Values":true},{"ID":107,"Name":"internal.metrics.input.recordsRead","Update":3,"Value":3,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0},"Task Metrics":{"Executor Deserialize Time":10,"Executor Deserialize CPU Time":10148306,"Executor Run Time":12,"Executor CPU Time":12020701,"Peak Execution Memory":0,"Result Size":1561,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":133,"Records Read":3},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"show at <console>:26","Number of Tasks":1,"RDD Info":[{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"23\",\"name\":\"mapPartitionsInternal\"}","Callsite":"show at <console>:26","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"22\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":8,"Name":"FileScanRDD","Scope":"{\"id\":\"22\",\"name\":\"Scan json \"}","Callsite":"show at <console>:26","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"15\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"show at <console>:26","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Barrier":false,"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.sql.Dataset.show(Dataset.scala:793)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:26)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:30)\n$line26.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:32)\n$line26.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:34)\n$line26.$read$$iw$$iw$$iw$$iw.<init>(<console>:36)\n$line26.$read$$iw$$iw$$iw.<init>(<console>:38)\n$line26.$read$$iw$$iw.<init>(<console>:40)\n$line26.$read$$iw.<init>(<console>:42)\n$line26.$read.<init>(<console>:44)\n$line26.$read$.<init>(<console>:48)\n$line26.$read$.<clinit>(<console>)\n$line26.$eval$.$print$lzycompute(<console>:7)\n$line26.$eval$.$print(<console>:6)\n$line26.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nscala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)","Submission Time":1651187568103,"Completion Time":1651187568142,"Accumulables":[{"ID":79,"Name":"duration","Value":"6","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":80,"Name":"number of output rows","Value":"2","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":81,"Name":"number of output rows","Value":"3","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":85,"Name":"internal.metrics.executorDeserializeTime","Value":10,"Internal":true,"Count Failed Values":true},{"ID":86,"Name":"internal.metrics.executorDeserializeCpuTime","Value":10148306,"Internal":true,"Count Failed Values":true},{"ID":87,"Name":"internal.metrics.executorRunTime","Value":12,"Internal":true,"Count Failed Values":true},{"ID":88,"Name":"internal.metrics.executorCpuTime","Value":12020701,"Internal":true,"Count Failed Values":true},{"ID":89,"Name":"internal.metrics.resultSize","Value":1561,"Internal":true,"Count Failed Values":true},{"ID":106,"Name":"internal.metrics.input.bytesRead","Value":133,"Internal":true,"Count Failed Values":true},{"ID":107,"Name":"internal.metrics.input.recordsRead","Value":3,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0}}
{"Event":"SparkListenerJobEnd","Job ID":2,"Completion Time":1651187568142,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":1,"time":1651187568174}
{"Event":"SparkListenerApplicationEnd","Timestamp":1651187579832}
