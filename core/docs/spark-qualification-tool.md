---
layout: page
title: Qualification Tool
nav_order: 8
---
# Spark Rapids Qualification Tool

The Qualification tool analyzes events generated by CPU based Spark applications to help quantify the expected acceleration and benefits of using the RAPIDS Accelerator for Apache Spark. Qualification results are intended to give users a starting point for optimization. Queries and and applications _recommended_ for GPU acceleration are not guaranteed to see acceleration.

The tool analyzes the CPU event log to determine which operations are likely to run on the GPU and estimates a speed-up for each operator based on its historical speed in queries and benchmarks. A final _"Estimated GPU App Duration"_ is calculated by adding the accelerated GPU operator durations and the original durations of unaccelerated operators that are either unsupported or not SQL/Dataframe operations.

GPU acceleration estimates are available for different CSP environments based on benchmarks run in the corresponding environment. ETL benchmarks used in GPU duration estimation are run using the [NDS benchmark](https://github.com/NVIDIA/spark-rapids-benchmarks/tree/dev/nds) at SF3K scale (3 TB) with the following cluster configurations:

| Environment      | CPU Cluster       | GPU Cluster                    |
|------------------|-------------------|--------------------------------|
| On-prem          | 8x 128-core       | 8x 128-core + 8x A100 40 GB    |
| Dataproc (T4)    | 4x n1-standard-32 | 4x n1-standard-32 + 8x T4 16GB |
| Dataproc (L4)    | 8x n1-standard-16 | 8x g2-standard-16              |
| EMR              | 8x m5d.8xlarge    | 4x g4dn.12xlarge               |
| Databricks AWS   | 8x m6gd.8xlage    | 8x g5.8xlarge                  |
| Databricks Azure | 8x E8ds_v4        | 8x NC8as_T4_v3                 |

> **Disclaimer!**  
> Estimates provided by the Qualification tool assume the application is run on a dedicated cluster where it can use all available Spark resources. The estimates are based on the "_SparkPlan_" or "_Executor Nodes_"
> used in the Spark application and currently supported by the RAPIDS Accelerator for Apache Spark. Not all expressions and datatypes are currently supported,
> Please refer to the "[Understanding Execs report](#execs-report)" section and the
> "[Supported Operators](https://github.com/NVIDIA/spark-rapids/blob/main/docs/supported_ops.md)" guide to verify the types and expressions used are supported.

## Table of Contents

- [Running the Qualification Tool](#Running-the-Qualification-Tool)
   - [Standalone on Spark event logs for CSP environments](#running-the-standalone-qualification-tool-for-csp-environments-on-spark-event-logs)
   - [Standalone on Spark event logs for other environments](#running-the-qualification-tool-standalone-on-spark-event-logs)
   - [Inside a running Spark application using the API](#running-the-qualification-tool-inside-a-running-spark-application-using-the-api)
   - [On active jobs using a Spark Listener](#running-the-qualification-tool-using-a-spark-listener)
- [Understanding the Qualification Tool Output](#understanding-the-qualification-tool-output)
   - [Application Report](#application-report)
   - [Stages Report](#stages-report)
   - [Execs Report](#execs-report)
   - [MLFunctions Report](#mlfunctions-report)
   - [MLFunctions Total Duration Report](#mlfunctions-total-duration-report)
   - [HTML Report](#html-report)
- [Example Reports and File Output](#example-reports-and-file-output)
- [Compiling the Tools Jar from Source](#compiling-the-tools-jar-from-source)
- [S3 File Path Support and Configuration](#s3-file-path-support-and-configuration)
- [Tools Jar Maven Dependency](#qualification-tool-maven-dependency-snippet)



## Running the Qualfication Tool

The Qualification tool can be run three different ways:
1. A standalone application running on Spark event logs after the applications are done
2. Integrated into a running Spark application using explicit API calls
3. Installing a Spark listener to output results on a per SQL query basis

Standalone mode can be run as a java application or using the `spark_rapids_user_tools` application that is available as the `spark-rapids-user-tools` [pip package](https://pypi.org/project/spark-rapids-user-tools/).

## Running the Standalone Qualification Tool for CSP Environments on Spark Event Logs
### User Tools Prerequisites and Setup for CSP Environments

* [Dataproc](../../user_tools/docs/user-tools-dataproc.md)
* [EMR](../../user_tools/docs/user-tools-aws-emr.md)
* [Databricks AWS](../../user_tools/docs/user-tools-databricks-aws.md)

### Qualify CPU Workloads for Potential Cost Savings and Acceleration with GPUs

The currently supported CSPs are *dataproc*, *emr*, and *databricks-aws*. The Qualification tool will run against logs from the specific CSP environment and will output GPU acceleration recomendations with estimated speed-up and cost saving metrics.

Usage:
```bash
# CSP provider ID
export CSP=<dataproc | emr | databricks-aws>
# The live cluster's name or the cluster configuration file detailing cluster instances and cluster size
export CLUSTER=<CSP hosted cluster>
# Storage location for Spark event logs
# For Dataproc, a GCS path
# For EMR or Databricks-AWS, a S3 path
export EVENTLOGS_PATH=<path to application event logs>

spark_rapids_user_tools ${CSP} qualification \
   --cpu_cluster ${CLUSTER} \
   --eventlogs ${EVENTLOGS_PATH}
```

> NOTE: If using S3, make sure the additional [S3 configuration steps](#S3-file-path-support-and-configuration) have been completed. More CSP specific configuration details are covered in the [linked documentation](#User-Tools-Prerequisites-and-Setup-for-CSP-Environments) for CSP specific configuration.

Example output:
```
+----+------------+--------------------------------+----------------------+-----------------+-----------------+---------------+-----------------+
|    | App Name   | App ID                         | Recommendation       |   Estimated GPU |   Estimated GPU |           App |   Estimated GPU |
|    |            |                                |                      |         Speedup |     Duration(s) |   Duration(s) |      Savings(%) |
|----+------------+--------------------------------+----------------------+-----------------+-----------------+---------------+-----------------|
|  0 | query24    | application_1664888311321_0011 | Strongly Recommended |            3.49 |          257.18 |        897.68 |           59.70 |
|  1 | query78    | application_1664888311321_0009 | Strongly Recommended |            3.35 |          113.89 |        382.35 |           58.10 |
|  2 | query23    | application_1664888311321_0010 | Strongly Recommended |            3.08 |          325.77 |       1004.28 |           54.37 |
|  3 | query64    | application_1664888311321_0008 | Strongly Recommended |            2.91 |          150.81 |        440.30 |           51.82 |
|  4 | query50    | application_1664888311321_0003 | Recommended          |            2.47 |          101.54 |        250.95 |           43.08 |
|  5 | query16    | application_1664888311321_0005 | Recommended          |            2.36 |          106.33 |        251.95 |           40.63 |
|  6 | query38    | application_1664888311321_0004 | Recommended          |            2.29 |           67.37 |        154.33 |           38.59 |
|  7 | query87    | application_1664888311321_0006 | Recommended          |            2.25 |           75.67 |        170.69 |           37.64 |
|  8 | query51    | application_1664888311321_0002 | Recommended          |            1.53 |           53.94 |         82.63 |            8.18 |
+----+------------+--------------------------------+----------------------+-----------------+-----------------+---------------+-----------------+
```

Help (to list all options available):
```bash
spark_rapids_user_tools ${CSP} qualification --help
```

## Running the Qualification Tool Standalone on Spark Event Logs

### Prerequisites
- Java 8 or above
- Spark 3.0.1+ jars
   - The Spark jars are required, but not the Apache Spark runtime.
   - If you do not already have Spark 3.x installed, download the Spark distribution from 
- Spark event log(s) from Spark 2.0+ in the following supported formats:
   - Rolled and compressed event logs with `.lz4`, `.lzf`, `.snappy` and `.zstd` suffixes.
   - Databricks-specific rolled and compressed `.gz` event logs.
   - The Qualification tool does not support nested directories. Event log files or event log directories should be at the top level when specifying a directory.
   - Spark event logs can be downloaded using the `Download` button in the Spark UI or in the location specified by the `spark.eventLog.dir` configuration. See the [Apache Spark Monitoring](http://spark.apache.org/docs/latest/monitoring.html) documentation for more information.
- Spark Rapids Tools jar installed
   - Download the latest tools jar from [this Maven repository](https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark-tools_2.12/).
   - To build the jar from source use [these instructions](#compiling-the-tools-jar-from-source).

### Qualification Tool Options
This list of options is available via the `--help` flag

```bash
java -cp ~/rapids-4-spark-tools_2.12-${TOOL_VERSION}.jar:${SPARK_HOME}/jars/* \
   com.nvidia.spark.rapids.tool.qualification.QualificationMain \
   --help
```

| <div style="width:149px">option</div> | Alias | Description | Default |
|--------|-------|-------------|--------|
| --all | | Apply multiple event log filtering criteria and process only logs for which all conditions are satisfied. Example: `<Filter1> <Filter2> <Filter3> --all` result is `<Filter1> AND <Filter2> AND <Filter3>` | all=true |
| --any | | Apply multiple event log filtering criteria and process only logs for which any condition is satisfied. Example: `<Filter1> <Filter2> <Filter3> --any` -> result is `<Filter1> OR <Filter2> OR <Filter3>` | any=false |
| --application-name \<arg> | -a | Filter event logs by  application name. The string specified can be a regular expression, substring, or exact match. For filtering based on complement of application name, use ~APPLICATION_NAME. i.e Select all event logs except the ones which have application name as the input string | |
| --filter-criteria \<arg> | -f | Filter newest or oldest N eventlogs based on application start timestamp, unique application name or filesystem timestamp. Filesystem based filtering happens before any application based filtering.For application based filtering, the order in which filters areapplied is: application-name, start-app-time, filter-criteria.Application based filter-criteria are:100-newest (for processing newest 100 event logs based on timestamp insidethe eventlog) i.e application start time)  100-oldest (for processing oldest 100 event logs based on timestamp insidethe eventlog) i.e application start time)  100-newest-per-app-name (select at most 100 newest log files for each unique application name) 100-oldest-per-app-name (select at most 100 oldest log files for each unique application name)Filesystem based filter criteria are:100-newest-filesystem (for processing newest 100 event logs based on filesystem timestamp). 100-oldest-filesystem (for processing oldest 100 event logsbased on filesystem timestamp). | |
| --html-report | -h | Generate an HTML report. | |
| --no-html-report | | Disable generating the HTML report. | |
| --match-event-logs \<arg> | -m | Filter event logs whose filenames contain the input string. Filesystem based filtering happens before any application based filtering. | |
| --max-sql-desc-length  \<arg> | | Maximum length of the SQL description string output with the per sql output. Default is 100. | |
| --ml-functions | | Report if there are any SparkML or Spark XGBoost functions in the eventlog. | |
| --num-output-rows \<arg> | -n | Number of output rows in the summary report. Default is 1000. | |
| --num-threads \<arg> | | Number of thread to use for parallel processing. The default is the number of cores on host divided by 4. | |
| --order \<arg> | | Specify the sort order of the report. desc or asc, desc is the default. desc (descending) would report applications most likely to be accelerated at the top and asc (ascending) would show the least likely to be accelerated at the top. | |
| --output-directory  \<arg> | -o | Base output directory. Default is current directory for the default filesystem. The final output will go into a subdirectory called rapids_4_spark_qualification_output. It will overwrite any existing directory with the same name. | |
| --per-sql | -p | Report at the individual SQL query level. | |
| --platform \<arg> | | Cluster platform where Spark CPU workloads were executed. Options include onprem, dataproc-t4, dataproc-l4, emr, databricks-aws, and databricks-azure. Default is onprem. | |
| --report-read-schema | -r | Whether to output the read formats and datatypes to the CSV file. This can be very long. Default is false. | |
| --spark-property  \<args> | | Filter applications based on certain Spark properties that were set during launch of the application. It can filter based on key:value pair or just based on keys. Multiple configs can be provided where the filtering is done if any of theconfig is present in the eventlog. filter on specific configuration: --spark-property=spark.eventLog.enabled:truefilter all eventlogs which has config: --spark-property=spark.driver.portMultiple configs: --spark-property=spark.eventLog.enabled:true --spark-property=spark.driver.port | |
| --start-app-time  \<arg> | -s | Filter event logs whose application start occurred within the past specified time period. Valid time periods are min(minute),h(hours),d(days),w(weeks),m(months). If a period is not specified it defaults to days. | |
| --timeout  \<arg> | -t | Maximum time in seconds to wait for the event logs to be processed. Default is 24 hours (86400 seconds) and must be greater than 3 seconds. If it times out, it will report what it was able to process up until the timeout. | |
| --user-name  \<arg> | -u | Applications which a particular user has submitted. | |
| --help | | Show help message | |

### Required Arguments

| Argument Name | Description|
|---------------|------------|
| eventlog | Event log filenames(space separated) or directories containing event logs. eg: `s3a://${BUCKET}/eventlog1 /path/to/eventlog2`

### Configuring the Qualification Tool

To accomodate the Qualification tool reading and processing log files in-memory we recommend passing the VM option `-Xmx10g`. Adjust as needed based on the number of applications and size of log files being processed.
```bash
export QUALIFICATION_HEAP=-Xmx10g
```

### Running the Qualification Tool
```bash
export QUALIFICATION_HEAP="-Xmx10g"
export LOG_PATH=<eventlogs | eventlog directories ...>
export SPARK_HOME=<path to spark distribution>
export TOOL_VERSION=<version>
export TOOL_JAR_PATH=/path/to/rapids-4-spark-tools_2.12-${TOOL_VERSION}.jar
export TOOL_OPTIONS=""
export TOOL_CLASSPATH=${TOOL_JAR_PATH}:${SPARK_HOME}/jars/*

java ${QUALIFICATION_HEAP} \
   -cp ${TOOL_CLASSPATH} \
   com.nvidia.spark.rapids.tool.qualification.QualificationMain \
   ${TOOL_OPTIONS} \
   ${LOG_PATH}
```

For event logs stored on an On-Premises HDFS Cluster include `${HADOOP_CONF_DIR}` in classpath, give the HDFS path and -- in the likely case that the default filesystem is HDFS -- augment any local file paths to include the `file:` prefix.

```bash
export TOOL_CLASSPATH=${TOOL_JAR_PATH}:${SPARK_HOME}/jars/*:${HADOOP_CONF_DIR}
```

### Example Qualification Tool Commands

- Process the 10 newest logs and only output the top 3 in the output:

```bash
java ${QUALIFICATION_HEAP} \
  -cp  ${TOOL_CLASSPATH} \
  com.nvidia.spark.rapids.tool.qualification.QualificationMain \
  -f 10-newest -n 3 ${LOG_PATH}
```

- Process last 100 days' logs:

```bash
java ${QUALIFICATION_HEAP} \
  -cp ${TOOL_CLASSPATH} \
  com.nvidia.spark.rapids.tool.qualification.QualificationMain \
  -s 100d ${LOG_PATH}
```

- Process only the newest log with the same application name:

```bash
java ${QUALIFICATION_HEAP} \
  -cp ${TOOL_CLASSPATH} \
  com.nvidia.spark.rapids.tool.qualification.QualificationMain \
  -f 1-newest-per-app-name ${LOG_PATH}
```

- Parse ML functions from the eventlog:

```bash
java ${QUALIFICATION_HEAP} \
  -cp ${TOOL_CLASSPATH} \
  com.nvidia.spark.rapids.tool.qualification.QualificationMain \
  --ml-functions ${LOG_PATH}
```

> NOTE: The “regular expression” used by the `-a` option is based on
[java.util.regex.Pattern](https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html).

### Qualification Tool Output

After one of the above commands are executed, the summary report goes to `STDOUT` and detailed CSV/log reports are written to `./rapids_4_spark_qualification_output/` in the default filesystem. The CSV reports include the estimated GPU performance of the app for each of the following:
_app execution_; _stages_; and _execs_. Currently only HDFS and the local filesystem are supported output filesystems.

> NOTE: Starting with release _`22.06`_ default behavior generates the report into HTML in addition to text files.

The tree structure of the output directory `${OUTPUT_FOLDER}/rapids_4_spark_qualification_output` is as follows:

```bash
    rapids_4_spark_qualification_output
    ├── rapids_4_spark_qualification_output.csv
    ├── rapids_4_spark_qualification_output.log
    ├── rapids_4_spark_qualification_output_persql.log
    ├── rapids_4_spark_qualification_output_persql.csv
    ├── rapids_4_spark_qualification_output_execs.csv
    ├── rapids_4_spark_qualification_output_stages.csv
    ├── rapids_4_spark_qualification_output_mlfunctions.csv
    ├── rapids_4_spark_qualification_output_mlfunctions_totalduration.csv
    └── ui
        ├── assets
        │   ├── bootstrap/
        │   ├── datatables/
        │   ├── jquery/
        │   ├── mustache-js/
        │   └── spur/
        ├── css
        │   └── rapids-dashboard.css
        ├── html
        │   ├── application.html
        │   ├── index.html
        │   ├── raw.html
        │   └── sql-recommendation.html
        └── js
            ├── app-report.js
            ├── data-output.js
            ├── per-sql-report.js            
            ├── qual-report.js
            ├── raw-report.js
            ├── ui-config.js
            └── uiutils.js
```

For information on individual file contents or processing the Qualification report and recommendations, please refer
to [ Understanding the Qualification Tool File Output](#Understanding-the-Qualification-tool-file-output) and
[Output Formats](#output-formats) sections.

## Running the Qualification Tool Using a Spark Listener

The Qualification tool can be added as a Spark listener when launching a Spark application or Spark shell to analyze each SQL query ran.

### Prerequisites
- Java 8 or above
- Spark 3.0.1+
   - If you do not already have Spark 3.x installed, download the Spark distribution from
- Spark Rapids Tools jar installed
   - Download the latest tools jar from [this Maven repository](https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark-tools_2.12/).
   - To build the jar from source use [these instructions](#compiling-the-tools-jar-from-source).

### Spark Listener Configuration

Add the `RunningQualificationEventProcessor` to the Spark Listen configuration:
```bash
--conf spark.extraListeners=org.apache.spark.sql.rapids.tool.qualification.RunningQualificationEventProcessor
```

#### Additional Configurations

By default, the Qualification output can be found in the Spark driver logs. If the output directory is specified, pairs of log and CSV file are written to the configured directory with support for local or distributed filesystem and blobstores like GCS or S3.
 - `spark.rapids.qualification.outputDir`

By default, each output file contains Qualification output for 10 SQL queries per file and up to 100 files will be kept at a time. This ensures results become available every 10th query completion rather than when the entire applications finishes when working with blob stores that won't show files until they are completely written. This behavior can be configured using the following configurations.
 - `spark.rapids.qualification.output.numSQLQueriesPerFile` - default 10
 - `spark.rapids.qualification.output.maxNumFiles` - default 100

### Run a Spark Shell with Qualification Listener

Run a Spark shell with the `RunningQualificationEventProcessor` listener configured, and other optional IO configurations.

Usage:
```bash
${SPARK_HOME}/bin/spark-shell \
   --jars rapids-4-spark-tools_2.12-${TOOLS_VERSION}.jar \
   --conf spark.extraListeners=org.apache.spark.sql.rapids.tool.qualification.RunningQualificationEventProcessor \
   --conf spark.rapids.qualification.outputDir=/tmp/qualPerSqlOutput \
   --conf spark.rapids.qualification.output.numSQLQueriesPerFile=5 \
   --conf spark.rapids.qualification.output.maxNumFiles=10
```

After running some SQL queries in the shell, the output directory should have files like:

```
rapids_4_spark_qualification_output_persql_0.csv
rapids_4_spark_qualification_output_persql_0.log
rapids_4_spark_qualification_output_persql_1.csv
rapids_4_spark_qualification_output_persql_1.log
rapids_4_spark_qualification_output_persql_2.csv
rapids_4_spark_qualification_output_persql_2.log
```

For information on file contents, please refer to the [ Understanding the Qualification Tool File Output](#Understanding-the-Qualification-tool-file-output) section.

## Running the Qualification tool inside a running Spark application using the API

### Prerequisites
- Java 8 or above
- Spark 3.0.1+
   - If you do not already have Spark 3.x installed, download the Spark distribution from
- Spark Rapids Tools jar installed
   - Download the latest tools jar from [this Maven repository](https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark-tools_2.12/).
   - To build the jar from source use [these instructions](#compiling-the-tools-jar-from-source).

### Augmenting Application Code with Qualification Tool APIs

Currently only Scala APIs are supported. 

> NOTE: Reporting on a per SQL query basis within an application is not currently supported. Analysis of an individual SQL query can be achieved manually by wrapping and reporting around the individual query instead of the entire application.

Create the `RunningQualicationApp`:
```Scala
val qualApp = new com.nvidia.spark.rapids.tool.qualification.RunningQualificationApp()
```

Get the event listener and install it as a Spark listener:
```Scala
val listener = qualApp.getEventListener
spark.sparkContext.addSparkListener(listener)
```

Run queries and then get the summary or detailed output to see the results.

The summary output api:
```Scala
/**
 * Get the summary report for qualification.
 * @param delimiter The delimiter separating fields of the summary report.
 * @param prettyPrint Whether to including the separate at start and end and
 *                    add spacing so the data rows align with column headings.
 * @return String of containing the summary report.
 */
getSummary(delimiter: String = "|", prettyPrint: Boolean = true): String
```

The detailed output api:
```Scala
/**
 * Get the detailed report for qualification.
 * @param delimiter The delimiter separating fields of the summary report.
 * @param prettyPrint Whether to including the separate at start and end and
 *                    add spacing so the data rows align with column headings.
 * @return String of containing the detailed report.
 */
getDetailed(delimiter: String = "|", prettyPrint: Boolean = true, reportReadSchema: Boolean = false): String
```

Example:
```Scala
// run your sql queries ...

// To get the summary output:
val summaryOutput = qualApp.getSummary()

// To get the detailed output:
val detailedOutput = qualApp.getDetailed()

// print the output somewhere for user to see
println(summaryOutput)
println(detailedOutput)
```

### Running the Spark Application

Execute your augmented Spark application, making sure to include the Spark Rapids Tools jar. Results can be found wherever the application was programmed to write them.

For example, if running the spark-shell:
```bash
${SPARK_HOME}/bin/spark-shell --jars rapids-4-spark-tools_2.12-<version>.jar
```

> NOTE: If the Spark application is compiled, the Spark Rapids Tools jar can be included using [this pom snippet](#qualification-tool-maven-dependency-snippet).

## Understanding the Qualification Tool Output

For each processed Spark application, the Qualification tool generates two main fields to help quantify the expected benefit of migrating a Spark application or query to GPU.

1. `Estimated GPU Duration`: the predicted runtime of the app if it was run on GPU. The sum of the accelerated operator durations, ML function durations (if applicable), and the durations of anything that could not run on GPU because they are unsupported operators or not SQL/Dataframe.
2. `Estimated Speed-up`: approximately how much faster the application would be on GPU using the original CPU duration of the app divided by the _Estimated GPU Duration_.

The lower the estimated GPU duration, the higher the `Estimated Speed-up`.
The processed applications or queries are sorted by the `Estimated Speed-up` and then classified into the following categories:

- `Strongly Recommended`
- `Recommended`
- `Not Recommended`
- `Not Applicable`: indicates that the app has job or stage failures.

As mentioned above, queries and and applications _recommended_ for GPU acceleration are not guaranteed to see acceleration. The estimates are based on the "_SparkPlan_" or "_Executor Nodes_" used in the Spark application and currently supported by the RAPIDS Accelerator for Apache Spark. Not all expressions and datatypes are currently supported, please refer to [Supported Operators](https://github.com/NVIDIA/spark-rapids/blob/main/docs/supported_ops.md) for the latest type and expression support information.

In addition to the _recommendation_, the Qualification tool reports a set of metrics in the tasks of SQL Dataframe operations
within the scope of the Application, Stages, and Execs. The report is divided into three main levels. The fields
of each level are described in details in the following sections: [Detailed App Report](#detailed-app-report),
[Stages report](#stages-report), and [Execs report](#execs-report). Then we describe the output formats and their file
locations in [Output Formats](#output-formats) section.

There is an option to print a report at the SQL query level in addition to the application level.

- [Application Report](#application-report)
- [Stages Report](#stages-report)
- [Execs Report](#execs-report)
- [MLFunctions Report](#mlfunctions-report)
- [MLFunctions Total Duration Report](#mlfunctions-total-duration-report)
- [HTML Report](#html-report)
   - [Application Recommendations Summary](#application-recommendations-summary)
   - [App Details View](#app-details-view)
   - [Raw Data](#raw-data)
   - [Per SQL Data](#per-sql-data)

### Application Report

The report represents the entire app execution, including unsupported operators and non-SQL operations.

| Field | Description |
|-------|-------------|
| _App Name_ | |
| _App ID_ | |
| _Recommendation_ | Recommendation based on `Estimated Speed-up Factor`, where an app can be "_Strongly Recommended_", "_Recommended_", "_Not Recommended_", or "_Not Applicable_". The latter indicates that the app has job or stage failures. |
| _App Duration_ | Wall-Clock time measured since the application starts till it is completed. If an app is not completed an estimated completion time would be computed. |
| _SQL DF duration_ | Wall-Clock time duration that includes only SQL-Dataframe queries. |
| _GPU Opportunity_ | Wall-Clock time that shows how much of the SQL duration and ML functions(if applicable) can be accelerated on the GPU. |
| _Estimated GPU Duration_ | Predicted runtime of the app if it was run on GPU. It is the sum of the accelerated operator durations and ML functions durations(if applicable) along with durations that could not run on GPU because they are unsupported operators or not SQL/Dataframe. |
| _Estimated GPU Speed-up_ | The speed-up is simply the original CPU duration of the app divided by the estimated GPU duration. That will estimate how much faster the application would run on GPU. |
| _Estimated GPU Time Saved_ | Estimated wall-Clock time saved if it was run on the GPU. |
| _SQL Dataframe Task Duration_ | Amount of time spent in tasks of SQL Dataframe operations. |
| _Executor CPU Time Percent_ | This is an estimate at how much time the tasks spent doing processing on the CPU vs waiting on IO. This is not always a good indicator because sometimes the IO that is encrypted and the CPU has to do work to decrypt it, so the environment you are running on needs to be taken into account. |
| _SQL Ids with Failures_ | SQL Ids of queries with failed jobs. |
| _Unsupported Read File Formats and Types_ | Looks at the Read Schema and reports the file formats along with types which may not be fully supported. Example | `JDBC[*]`. Note that this is based on the current version of the plugin and future versions may add support for more file formats and types. |
| _Unsupported Write Data Format_ | Reports the data format which we currently don’t support, i.e. if the result is written in JSON or CSV format. |
| _Complex Types_ | Looks at the Read Schema and reports if there are any complex types(array, struct or maps) in the schema. |
| _Nested Complex Types_ | Nested complex types are complex types which contain other complex types (Example | `array<struct<string,string>>`). Note that it can read all the schemas for DataSource V1. The Data Source V2 truncates the schema, so if you see "`...`", then the full schema is not available. For such schemas we read until `...` and report if there are any complex types and nested complex types in that. |
| _Potential Problems_ | Some UDFs and nested complex types. Please keep in mind that the tool is only able to detect certain issues. |
| _Longest SQL Duration_ | The maximum amount of time spent in a single task of SQL Dataframe operations. |
| _NONSQL Task Duration Plus Overhead_ | Time duration that does not span any running SQL task. |
| _Unsupported Task Duration_ | Sum of task durations for any unsupported operators. |
| _Supported SQL DF Task Duration_ | Sum of task durations that are supported by RAPIDS GPU acceleration. |
| _Task Speedup Factor_ | The average speed-up of all stages. |
| _App Duration Estimated_ | True or False indicates if we had to estimate the application duration. If we had to estimate it, the value will be `True` and it means the event log was missing the application finished event, so we will use the last job or sql execution time we find as the end time used to calculate the duration. |
| _Unsupported Execs_ | Reports all the execs that are not supported by GPU in this application. Note that an Exec name may be  printed in this column if any of the expressions within this Exec is not supported by GPU. If the resultant string exceeds maximum limit (25), then ... is suffixed to the STDOUT and full output can be found in the CSV file. |
| _Unsupported Expressions_ | Reports all expressions not supported by GPU in this application. |
| _Read Schema_ | Shows the datatypes and read formats. This field is only listed when the argument `--report-read-schema` is passed to the CLI. |
| _Estimated Frequency_ | Application executions per month assuming uniform distribution, default frequency is daily (30 times per month) and minimum frequency is monthly (1 time per month). For a given log set, determines a logging window using the earliest start time and last end time of all logged applications. Counts the number of executions of a specific `App Name` over the logging window and converts the frequency to per month (30 days). Applications that are only ran once are assigned the default frequency. |

> NOTE: the Qualification tool won't catch all UDFs and some UDFs can be handled with additional steps.
Please refer to [Supported Operators](https://github.com/NVIDIA/spark-rapids/blob/main/docs/supported_ops.md) for more details on UDF support.

By default, the applications and queries are sorted in descending order by the following fields:
- _Recommendation_;
- _Estimated GPU Speed-up_;
- _Estimated GPU Time Saved_; and
- _End Time_.

### Stages Report

For each stage used in SQL operations, the Qualification tool generates the following information:

| Field | Description |
|-------|-------------|
| _App ID_ | |
| _Stage ID_ | |
| _Average Speedup Factor_ | The average estimated speed-up of all the operators in the given stage. |
| _Stage Task Duration_ | Amount of time spent in tasks of SQL Dataframe operations for the given stage. |
| _Unsupported Task Duration_ | Sum of task durations for the unsupported operators. For more details, see [Supported Operators](https://github.com/NVIDIA/spark-rapids/blob/main/docs/supported_ops.md). |
| _Stage Estimated_ | True or False indicates if we had to estimate the stage duration. |

### Execs Report

The Qualification tool generates a report of the `Exec` in the `SparkPlan` or `Executor Nodes` along with the estimated acceleration on the GPU. Please refer to the [Supported Operators](https://github.com/NVIDIA/spark-rapids/blob/main/docs/supported_ops.md) guide for more details on limitations on UDFs and unsupported operators.

| Field | Description |
|-------|-------------|
| _App ID_ | |
| _SQL ID_ | |
| _Exec Name_ | example `Filter`, `HashAggregate` |
| _Expression Name_ | |
| _Task Speedup Factor_ | The average acceleration of the operators based on the original CPU duration of the operator divided by the GPU duration. The tool uses historical queries and benchmarks to estimate a speed-up at an individual operator level to calculate how much a specific operator would accelerate on GPU. |
| _Exec Duration_ | Wall-Clock time measured since the operator starts till it is completed. |
| _SQL Node Id_ | |
| _Exec Is Supported_ | Whether the Exec is supported by RAPIDS or not. Please refer to [Supported Operators](https://github.com/NVIDIA/spark-rapids/blob/main/docs/supported_ops.md) for more information. |
| _Exec Stages_ | An array of stage IDs |
| _Exec Children_ | |
| _Exec Children Node Ids_ | |
| _Exec Should Remove_ | Whether the Op is removed from the migrated plan. |

### Parsing Expressions Within Each Exec

The Qualification tool looks at the expressions in each `Exec` to provide a fine-grained assessment of RAPIDS Accelerator for Apache Spark support.
> NOTE: It is not possible to extract the expressions for each available _Exec_:
> - Some Execs do not take any expressions, and
> - Some execs may not show the expressions in the _eventlog_.

The following table lists the `Exec`'s name and the status of parsing their expressions:
- `Expressions Unavailable` marks the `Execs` that do not show expressions in the _eventlog_;
- `Fully Parsed` marks the `Execs` that have their expressions fully parsed by the Qualification tool;
- `In Progress` marks the `Execs` that are still being investigated; therefore, a set of the marked `Execs` may be fully parsed in future releases.

| **Exec**                              | **Expressions Unavailable** | **Fully Parsed** | **In Progress** |
|---------------------------------------|:---------------------------:|:----------------:|:---------------:|
| AggregateInPandasExec                 |              -              |         -        |        x        |
| AQEShuffleReadExec                    |              -              |         -        |        x        |
| ArrowEvalPythonExec                   |              -              |         -        |        x        |
| BatchScanExec                         |              -              |         -        |        x        |
| BroadcastExchangeExec                 |              -              |         -        |        x        |
| BroadcastHashJoinExec                 |              -              |         -        |        x        |
| BroadcastNestedLoopJoinExec           |              -              |         -        |        x        |
| CartesianProductExec                  |              -              |         -        |        x        |
| CoalesceExec                          |              -              |         -        |        x        |
| CollectLimitExec                      |              x              |         -        |        -        |
| CreateDataSourceTableAsSelectCommand  |              -              |         -        |        x        |
| CustomShuffleReaderExec               |              -              |         -        |        x        |
| DataWritingCommandExec                |              -              |         -        |        x        |
| ExpandExec                            |              -              |         -        |        x        |
| FileSourceScanExec                    |              -              |         -        |        x        |
| FilterExec                            |              -              |         x        |        -        |
| FlatMapGroupsInPandasExec             |              -              |         -        |        x        |
| GenerateExec                          |              -              |         -        |        x        |
| GlobalLimitExec                       |              x              |         -        |        -        |
| HashAggregateExec                     |              -              |         x        |        -        |
| InMemoryTableScanExec                 |              -              |         -        |        x        |
| InsertIntoHadoopFsRelationCommand     |              -              |         -        |        x        |
| LocalLimitExec                        |              x              |         -        |        -        |
| MapInPandasExec                       |              -              |         -        |        x        |
| ObjectHashAggregateExec               |              -              |         x        |        -        |
| ProjectExec                           |              -              |         x        |        -        |
| RangeExec                             |              x              |         -        |        -        |
| SampleExec                            |              -              |         -        |        x        |
| ShuffledHashJoinExec                  |              -              |         -        |        x        |
| ShuffleExchangeExec                   |              -              |         -        |        x        |
| SortAggregateExec                     |              -              |         x        |        -        |
| SortExec                              |              -              |         x        |        -        |
| SortMergeJoinExec                     |              -              |         -        |        x        |
| SubqueryBroadcastExec                 |              -              |         -        |        x        |
| TakeOrderedAndProjectExec             |              -              |         -        |        x        |
| UnionExec                             |              x              |         -        |        -        |
| WindowExec                            |              -              |         x        |        -        |
| WindowInPandasExec                    |              -              |         -        |        x        |

### MLFunctions Report

The Qualification tool generates this report if there are SparkML or Spark XGBoost functions used in the eventlog.
The functions in "*spark.ml.*" or "*spark.XGBoost.*" packages are displayed in the report.

| Field | Description |
|-------|-------------|
| _App ID_ | |
| _Stage ID_ | |
| _ML Functions_ | List of ML functions used in the corresponding stage. |
| _Stage Task Duration_ | Amount of time spent in tasks containing ML functions for the given stage. |

### MLFunctions Total Duration Report

The Qualification tool generates a report of total duration across all stages for ML functions that are supported on GPU.

| Field | Description |
|-------|-------------|
| _App Name_ | |
| _Stage_Ids_ | Stage Id's corresponding to the given ML function. |
| _ML Function Name_ | ML function name supported on GPU. |
| _Total Duration_ | total duration across all stages for the corresponding ML function. |

### HTML Report

Starting with release _`22.06`_, the HTML report is generated by default in a subdirecotry of the output directory 
`${OUTPUT_FOLDER}/rapids_4_spark_qualification_output/ui`. The HTML report can be disabled by passing the `--no-html-report` flag as described in the
[Qualification tool options](#Qualification-tool-options) section.

To browse the content of the html report:

1. For HDFS or a remote node, copy the `${OUTPUT_FOLDER}/rapids_4_spark_qualification_output/ui` directory to your local node.
2. Open `ui/index.html` using your local machine's web-browser (Chrome/Firefox are recommended).

The HTML view renders the detailed information into tables that allow:

- Searching
- Ordering by specific column
- Exporting the table to a CSV file
- Interactive filtering by recommendation and/or username.

By default, all tables show 20 entries per page, which can be changed by selecting a different page-size in the table's navigation bar.

### Application Recommendations Summary

`index.html` shows the summary of the estimated GPU performance. The "_GPU Recommendations Table_"
lists the processed applications ranked by the "_Estimated GPU Speed-up_" along with the ability to search, and filter
the results. By clicking the "_App ID_" link of a specific app, you navigate to the details view of that app which is
described in [App-Details View](#app-details-view) section.

The summary report contains the following components:

1. **Stats-Row**: statistics card summarizing the following information:
    1. "_Total Applications_": total number of applications analyzed by the Qualification tool and the total execution
       time.
    2. "_RAPIDS Candidates_": marks the number applications that are either "_Recommended_", or "_Strongly Recommended_".
    3. "_GPU Opportunity_": shows the total of "_GPU Opportunity_" and "_SQL DF duration_" fields across all the apps.
2. **GPU Recommendations Table**: this table lists all the analyzed applications along with subset of fields that are
   directly involved in calculating the GPU performance estimate. Each row expands showing more fields by clicking on
   the control column.
3. The _searchPanes_ with the capability to search the app list by selecting rows in the panes.
   The "_Recommendations_" and "_Spark User_" filters are cascaded which allows the panes to be filtered based on the
   values selected in the other pane.
4. Text Search field that allows further filtering, removing data from the result set as keywords are entered. The
   search box will match on multiple columns including: "_App ID_", "_App Name_", "_Recommendation_"
5. HTML5 export button saves the table to CSV file into the browser's default download folder.
6. The `Raw Data` link in the left navigation bar redirects to a detailed report.
7. The `Per-SQL Data` link in the left navigation bar redirects to a summary report that shows
   the _per-SQL_ estimated GPU performance.

![Qualification-HTML-Recommendation-View](img/Tools/qualification-tool-recommendation-indexview-with-persql.png)

### App-Details View

When you click the "_App ID_" of a specific row in the "_GPU Recommendations Table_", the browser navigates to
this view which shows the metrics and estimated GPU performance for the given application.
It contains the following main components:

1. **Card title**: contains the application name and the Recommendation.
2. **Stats-Row**: statistics card summarizing the following information:
    1. "_App Duration_": the total execution time of the app, marking the start and end time.
    2. "_GPU Opportunity_": the wall-Clock time that shows how much of the SQL duration can be accelerated on the GPU. It
       shows the actual wall-Clock time duration that includes only SQL-Dataframe queries including non-supported ops,
       dubbed "_SQL DF Duration_". This is followed by "_Task Speed-up Factor_" which represents the average speed-up
       of all app stages.
    3. "_Estimated GPU Duration_": the predicted runtime of the app if it was run on GPU. For convenience, it calculates
       the estimated wall-clock time difference between the CPU and GPU executions. The original CPU duration of the app
       divided by the estimated GPU duration and displayed as "_App Speed-up_".
3. **Application Details**: this table lists all the fields described previously in
   the [Detailed App report](#detailed-app-report) section. Note that this table has more columns than can fit in a
   normal browser window. Therefore, the UI
   application dynamically optimizes the layout of the table to fit the browser screen. By clicking on the control
   column, the row expands to show the remaining hidden columns.
   ![Qualification-HTML-App-Details-View-Header](img/Tools/qualification-tool-app-view-01.png)
4. **Stage Details Table**: lists all the app stages with set of columns listed in [Stages report](#stages-report)
   section. The HTML5 export button saves the table to CSV file into the browser's default download folder.
   ![Qualification-HTML-App-Details-View-Stages](img/Tools/qualification-tool-app-view-02.png)
   The table has cascaded _searchPanes_, which means that the table allows the panes
   to be filtered based on the values selected in the other panes.  
   There are three searchPanes:
    1. "_Is Stage Estimated_": it splits the stages into two groups based on whether the stage duration time was estimated
       or not.
    2. "_Speed-up_": groups the stages by their "average speed-up". Each stage can belong to one of the following
       predefined speed-up ranges: `1.0 (No Speed-up)`; `]1.0, 1.3[`; `[1.3, 2.5[`; `[2.5, 5[`; and `[5, _]`. The
       search-pane does not show a range bucket if its count is 0.
    3. "_Tasks GPU Support_": this filter can be used to find stages having all their execs supported by the GPU.
5. **Execs Details Table**: lists all the app Execs with set of columns listed in [Execs report](#execs-report)
   section. The HTML5 export button saves the table to CSV file into the browser's default
   download folder.
   ![Qualification-HTML-App-Details-View-Execs](img/Tools/qualification-tool-app-view-03.png)
   The table has cascaded _searchPanes_, which means that the table allows the panes
   to be filtered based on the values selected in the other panes.  
   There are three _searchPanes_:
    1. "_Exec_": filters the rows by exec name. This filter also allows text searching by typing into the filter-title as
       a text input.
    2. "_Speed-up_": groups the stages by their "average speed-up". Each stage can belong to one of the following
       predefined speed-up ranges: `1.0 (No Speed-up)`; `]1.0, 1.3[`; `[1.3, 2.5[`; `[2.5, 5[`; and `[5, _]`. The
       search-pane does not show a range bucket if its count is 0.
    3. "_GPU Support_": filters the execs whether an exec is supported by GPU or not.
    4. "_Stage ID_": filters rows by the stage ID. It also allows text-searching by typing into the filter-title as a text
       input.
    5. "_Is Exec Removed_": filters rows that were removed from the migrated plan.
    6. **SQL Details Table**: lists _Per-SQL_ GPU recommendation. The HTML5 export button saves the table to CSV file into
       the browser's default download folder. The rows in the table can be filtered by "_SQL Description_", "_SQL ID_",
       or "_Recommendation_".

### Raw Data

`raw.html` displays all the fields listed in "_Detailed App Report_" in more readable format.
Columns representing "_time duration_" are rounded to nearest "ms", "seconds", "minutes", and "hours".
The search box will match on multiple columns including: "_App ID_", "_App Name_", "_Recommendation_",
"_User Name_", "_Unsupported Write Data Format_", "_Complex Types_", "_Nested Complex Types_", and "_Read Schema_".
The detailed table can also be exported as a CSV file into the browser's default download folder.

> NOTE: This table has more columns than can fit in a normal browser window. This causes the UI application to dynamically optimize the layout of the table to fit the browser. Click on the control column to expand the row and show the hidden columns.

### Per-SQL Data

`sql-recommendation.html` displays a summary of the estimate GPU performance for each query. Note that the
SQL queries across all the apps are combined in a single view; therefore, the "_SQL ID_" field may not be
unique. 

## Example Reports and File Output

The Qualification tool generates a set of CSV and log files in the output folder `${OUTPUT_FOLDER}/rapids_4_spark_qualification_output`.

> NOTE: Starting with release _`23.06`_, CSV output uses escapes and quotes to avoid parsing errors. Spark's default CSV escape character is the backslash(`\`), but standard spreadsheets use the double quote("). The Qualification tool output uses the double quote(`"`) escape character to avoid breaking spreadsheet software functionality. As a result, the data source must be configured using `option("ESCAPE", "\"")` when reading the Qualification tool CSV files into Spark.

### Application Report Summary

A brief summary that includes the application's projected GPU performance. This summary is sent to `STDOUT` in addition to being saved as `rapids_4_spark_qualification_output.log` and `rapids_4_spark_qualification_output.csv`.

The summary report outputs the following information:
"_App Name_", "_App ID_", "_App Duration_", "_SQL DF duration_", "_GPU Opportunity_", "_Estimated GPU Duration_", "_Estimated GPU Speed-up_", "_Estimated GPU Time Saved_", and "_Recommendation_".
Descriptions of these fields can be found in the [detailed application report](#detailed-application-report) section.

> NOTE: Durations are reported in milliseconds.

Sample log text output:
```
+------------+--------------+----------+----------+-------------+-----------+-----------+-----------+--------------------+-------------------------------------------------------+
|  App Name  |    App ID    |    App   |  SQL DF  |     GPU     | Estimated | Estimated | Estimated |  Recommendation    |      Unsupported Execs        |Unsupported Expressions|
|            |              | Duration | Duration | Opportunity |    GPU    |    GPU    |    GPU    |                    |                               |                       |
|            |              |          |          |             |  Duration |  Speedup  |    Time   |                    |                               |                       |
|            |              |          |          |             |           |           |   Saved   |                    |                               |                       |
+============+==============+==========+==========+=============+===========+===========+===========+====================+=======================================================+
| appName-01 | app-ID-01-01 |    898429|    879422|       879422|  273911.92|       3.27|  624517.06|Strongly Recommended|                               |                       |
+------------+--------------+----------+----------+-------------+-----------+-----------+-----------+--------------------+-------------------------------------------------------+
| appName-02 | app-ID-02-01 |      9684|      1353|         1353|    8890.09|       1.08|      793.9|     Not Recommended|Filter;SerializeFromObject;S...|           hex         |
+------------+--------------+----------+----------+-------------+-----------+-----------+-----------+--------------------+-------------------------------------------------------+
```

In the above example, two application event logs were analyzed. `app-ID-01-01` is _`Strongly Recommended`_
because its high `Estimated GPU Speedup` at `3.27`, but `app-ID-02-01` is _`Not Recommended`_ due to its low `Estimated GPU Speedup`.

### Per SQL Query Report Summary

The Qualification tool can optionally generate a report per SQL query. It generates a row similar to the above [application summary](#application-summary-report) for each SQL query. This per SQL summary is sent to `STDOUT` in addition to being saved as `rapids_4_spark_qualification_output_persql.log` and `rapids_4_spark_qualification_output_persql.csv`.

The summary report outputs the following information:
"_App Name_", "_App ID_", "_SQL ID_", "_SQL Description_", "_SQL DF duration_", "_GPU Opportunity_", "_Estimated GPU Duration_", "_Estimated GPU Speed-up_", "_Estimated GPU Time Saved_", and "_Recommendation_".
Descriptions of these fields can be found in the [detailed application report](#detailed-application-report) section.

> NOTE: Durations are reported in milliseconds.

Sample log text output:
```
+------------+--------------+----------+---------------+----------+-------------+-----------+-----------+-----------+--------------------+
|  App Name  |    App ID    |  SQL ID  |      SQL      |  SQL DF  |     GPU     | Estimated | Estimated | Estimated |  Recommendation    |
|            |              |          |  Description  | Duration | Opportunity |    GPU    |    GPU    |    GPU    |                    |
|            |              |          |               |          |             |  Duration |  Speedup  |    Time   |                    |
|            |              |          |               |          |             |           |           |   Saved   |                    |
+============+==============+==========+===============+==========+=============+===========+===========+===========+====================+
| appName-01 | app-ID-01-01 |         1|        query41|       571|          571|     187.21|       3.05|     383.78|Strongly Recommended|
+------------+--------------+----------+---------------+----------+-------------+-----------+-----------+-----------+--------------------+
| appName-02 | app-ID-02-01 |         3|        query44|      1116|            0|    1115.98|        1.0|       0.01|     Not Recommended|
+------------+--------------+----------+---------------+----------+-------------+-----------+-----------+-----------+--------------------+
```

Sample CSV output:
```
+---------------+-----------------------+------+----------------------------------------------------------+---------------+---------------+----------------------+---------------------+------------------------+--------------------+
|       App Name|                 App ID|SQL ID|                                           SQL Description|SQL DF Duration|GPU Opportunity|Estimated GPU Duration|Estimated GPU Speedup|Estimated GPU Time Saved|      Recommendation|
+===============+=======================+======+==========================================================+===============+===============+======================+=====================+========================+====================+
|NDS - Power Run|app-20220702220255-0008|   103|                                                   query87|          15871|          15871|               4496.03|                 3.53|                11374.96|Strongly Recommended|
|NDS - Power Run|app-20220702220255-0008|   106|                                                   query38|          11077|          11077|               3137.96|                 3.53|                 7939.03|Strongly Recommended|
+---------------+-----------------------+------+----------------------------------------------------------+---------------+---------------+----------------------+---------------------+------------------------+--------------------+
```

### Detailed Application Stages Report
`rapids_4_spark_qualification_output_stages.csv`.

Sample log text output:
```
+--------------+----------+-----------------+------------+---------------+-----------+
|    App ID    | Stage ID | Average Speedup | Stage Task |  Unsupported  |   Stage   |
|              |          |      Factor     |  Duration  | Task Duration | Estimated |
+==============+==========+=================+============+===============+===========+
| app-ID-01-01 |       25 |             2.1 |         23 |             0 |     false |
+--------------+----------+-----------------+------------+---------------+-----------+
| app-ID-02-01 |       29 |            1.86 |          0 |             0 |      true |
+--------------+----------+-----------------+------------+---------------+-----------+
```

### Detailed Application Execs Report
Similar to the app and stage information, this table shows estimated GPU performance of the SQL Dataframe operations and is saved as `rapids_4_spark_qualification_output_execs.csv`.

Sample log text output:
```
+--------------+--------+---------------------------+-----------------------+--------------+----------+----------+-----------+--------+----------------------------+---------------+-------------+
|    App ID    | SQL ID |         Exec Name         |    Expression Name    | Task Speedup |   Exec   | SQL Node |  Exec Is  |  Exec  |        Exec Children       | Exec Children | Exec Should |
|              |        |                           |                       |    Factor    | Duration |    Id    | Supported | Stages |                            |    Node Ids   |    Remove   |
+==============+========+===========================+=======================+==============+==========+==========+===========+========+============================+===============+=============+
| app-ID-02-01 |      7 | Execute CreateViewCommand |                       |          1.0 |        0 |        0 |     false |        |                            |               |       false |
+--------------+--------+---------------------------+-----------------------+--------------+----------+----------+-----------+--------+----------------------------+---------------+-------------+
| app-ID-02-01 |     24 |                   Project |                       |          2.0 |        0 |       21 |      true |        |                            |               |       false |
+--------------+--------+---------------------------+-----------------------+--------------+----------+----------+-----------+--------+----------------------------+---------------+-------------+
| app-ID-02-01 |     24 |              Scan parquet |                       |          2.0 |      260 |       36 |      true |     24 |                            |               |       false |
+--------------+--------+---------------------------+-----------------------+--------------+----------+----------+-----------+--------+----------------------------+---------------+-------------+
| app-ID-02-01 |     15 | Execute CreateViewCommand |                       |          1.0 |        0 |        0 |     false |        |                            |               |       false |
+--------------+--------+---------------------------+-----------------------+--------------+----------+----------+-----------+--------+----------------------------+---------------+-------------+
| app-ID-02-01 |     24 |                   Project |                       |          2.0 |        0 |       14 |      true |        |                            |               |       false |
+--------------+--------+---------------------------+-----------------------+--------------+----------+----------+-----------+--------+----------------------------+---------------+-------------+
| app-ID-02-01 |     24 |     WholeStageCodegen (6) | WholeStageCodegen (6) |          2.8 |      272 |        2 |      true |     30 | Project:BroadcastHashJoin: |         3:4:5 |       false |
|              |        |                           |                       |              |          |          |           |        |              HashAggregate |               |             |
+--------------+--------+---------------------------+-----------------------+--------------+----------+----------+-----------+--------+----------------------------+---------------+-------------+
```

## Compiling the Tools JAR from Source

[Building the tools jar](../README.md#build)

## S3 File Path Support and Configuration

There are 3 additional configuration steps necessary to support S3 file and directory paths in Spark:

1. For the desired Hadoop version, download and include  the [`hadoop-aws-<version>.jar`](https://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-aws/) and [`aws-java-sdk-<version>.jar`](https://repo.maven.apache.org/maven2/com/amazonaws/aws-java-sdk/) dependencies. For Hadoop version `2.7.4` see [hadoop-aws-2.7.4.jar](https://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-aws/2.7.4/hadoop-aws-2.7.4.jar) and [aws-java-sdk-1.7.4.jar](https://repo.maven.apache.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar).

2. Include the downloaded jars as part of your `spark-shell` or `spark-submit` command using the '--jars' option.

3. In `${SPARK_HOME}/conf`, create `hdfs-site.xml` and add user specific AWS S3 keys inside:

```xml
<?xml version="1.0"?>
<configuration>
<property>
  <name>fs.s3a.access.key</name>
  <value>xxx</value>
</property>
<property>
  <name>fs.s3a.secret.key</name>
  <value>xxx</value>
</property>
</configuration>
```

Please refer to [this doc](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html) for
more information about the `hadoop-aws` module and S3 integration.

## Qualification Tool Maven Dependency Snippet
If using the Qualification tool jar as a maven dependency:
```xml
<dependency>
   <groupId>com.nvidia</groupId>
   <artifactId>rapids-4-spark-tools_2.12</artifactId>
   <version>${version}</version>
</dependency>
```