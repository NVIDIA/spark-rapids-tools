<?xml version="1.0" encoding="UTF-8"?>
<!--
  Copyright (c) 2021-2025, NVIDIA CORPORATION.

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.nvidia</groupId>
    <artifactId>rapids-4-spark-tools_2.12</artifactId>
    <name>RAPIDS Accelerator for Apache Spark tools</name>
    <description>RAPIDS Accelerator for Apache Spark tools</description>
    <version>25.10.9-SNAPSHOT</version>
    <packaging>jar</packaging>
    <url>http://github.com/NVIDIA/spark-rapids-tools</url>

    <licenses>
        <license>
            <name>Apache License, Version 2.0</name>
            <url>https://www.apache.org/licenses/LICENSE-2.0.txt</url>
            <distribution>repo</distribution>
        </license>
    </licenses>
    <scm>
        <connection>scm:git:https://github.com/NVIDIA/spark-rapids-tools.git</connection>
        <developerConnection>scm:git:git@github.com:NVIDIA/spark-rapids-tools.git</developerConnection>
        <tag>HEAD</tag>
        <url>https://github.com/NVIDIA/spark-rapids-tools</url>
    </scm>
    <developers>
        <developer>
            <id>revans2</id>
            <name>Robert Evans</name>
            <email>roberte@nvidia.com</email>
            <roles>
                <role>Committer</role>
            </roles>
            <timezone>-6</timezone>
        </developer>
        <developer>
            <id>tgravescs</id>
            <name>Thomas Graves</name>
            <email>tgraves@nvidia.com</email>
            <roles>
                <role>Committer</role>
            </roles>
            <timezone>-6</timezone>
        </developer>
        <developer>
            <id>jlowe</id>
            <name>Jason Lowe</name>
            <email>jlowe@nvidia.com</email>
            <roles>
                <role>Committer</role>
            </roles>
            <timezone>-6</timezone>
        </developer>
    </developers>
    <profiles>
        <profile>
            <id>spark320</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>320</value>
                </property>
            </activation>
            <properties>
                <buildver>320</buildver>
                <spark.version>${spark320.version}</spark.version>
                <delta.core.version>${delta20x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark32x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark32x.runtime.version}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark321</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>321</value>
                </property>
            </activation>
            <properties>
                <buildver>321</buildver>
                <spark.version>${spark321.version}</spark.version>
                <delta.core.version>${delta20x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark32x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark32x.runtime.version}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark322</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>322</value>
                </property>
            </activation>
            <properties>
                <buildver>322</buildver>
                <spark.version>${spark322.version}</spark.version>
                <delta.core.version>${delta20x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark32x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark32x.runtime.version}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark323</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>323</value>
                </property>
            </activation>
            <properties>
                <buildver>323</buildver>
                <spark.version>${spark323.version}</spark.version>
                <delta.core.version>${delta20x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark32x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark32x.runtime.version}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark324</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>324</value>
                </property>
            </activation>
            <properties>
                <buildver>324</buildver>
                <spark.version>${spark324.version}</spark.version>
                <delta.core.version>${delta20x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark32x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark32x.runtime.version}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark325</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>325</value>
                </property>
            </activation>
            <properties>
                <buildver>325</buildver>
                <spark.version>${spark325.version}</spark.version>
                <delta.core.version>${delta20x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark32x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark32x.runtime.version}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark330</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>330</value>
                </property>
            </activation>
            <properties>
                <buildver>330</buildver>
                <spark.version>${spark330.version}</spark.version>
                <delta.core.version>${delta23x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark33x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark331</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>331</value>
                </property>
            </activation>
            <properties>
                <buildver>331</buildver>
                <spark.version>${spark331.version}</spark.version>
                <delta.core.version>${delta23x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark33x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark332</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>332</value>
                </property>
            </activation>
            <properties>
                <buildver>332</buildver>
                <spark.version>${spark332.version}</spark.version>
                <delta.core.version>${delta23x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark33x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark333</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>333</value>
                </property>
            </activation>
            <properties>
                <buildver>333</buildver>
                <spark.version>${spark333.version}</spark.version>
                <delta.core.version>${delta23x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark33x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark334</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>334</value>
                </property>
            </activation>
            <properties>
                <buildver>334</buildver>
                <spark.version>${spark334.version}</spark.version>
                <delta.core.version>${delta23x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark33x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark335</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>335</value>
                </property>
            </activation>
            <properties>
                <buildver>335</buildver>
                <spark.version>${spark335.version}</spark.version>
                <delta.core.version>${delta23x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark33x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark340</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>340</value>
                </property>
            </activation>
            <properties>
                <buildver>340</buildver>
                <spark.version>${spark340.version}</spark.version>
                <delta.core.version>${delta24x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark34x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark341</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>341</value>
                </property>
            </activation>
            <properties>
                <buildver>341</buildver>
                <spark.version>${spark341.version}</spark.version>
                <delta.core.version>${delta24x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark34x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark342</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>342</value>
                </property>
            </activation>
            <properties>
                <buildver>342</buildver>
                <spark.version>${spark342.version}</spark.version>
                <delta.core.version>${delta24x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark34x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark343</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>343</value>
                </property>
            </activation>
            <properties>
                <buildver>343</buildver>
                <spark.version>${spark343.version}</spark.version>
                <delta.core.version>${delta24x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark34x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark344</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>344</value>
                </property>
            </activation>
            <properties>
                <buildver>344</buildver>
                <spark.version>${spark344.version}</spark.version>
                <delta.core.version>${delta24x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark34x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark350</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>350</value>
                </property>
            </activation>
            <properties>
                <buildver>350</buildver>
                <spark.version>${spark350.version}</spark.version>
                <delta.core.artifactory>${delta.core.artifactory.post35}</delta.core.artifactory>
                <delta.core.version>${delta33x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark35x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark351</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>351</value>
                </property>
            </activation>
            <properties>
                <buildver>351</buildver>
                <spark.version>${spark351.version}</spark.version>
                <delta.core.artifactory>${delta.core.artifactory.post35}</delta.core.artifactory>
                <delta.core.version>${delta33x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark35x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark352</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>352</value>
                </property>
            </activation>
            <properties>
                <buildver>352</buildver>
                <spark.version>${spark352.version}</spark.version>
                <delta.core.artifactory>${delta.core.artifactory.post35}</delta.core.artifactory>
                <delta.core.version>${delta33x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark35x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark353</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>353</value>
                </property>
            </activation>
            <properties>
                <buildver>353</buildver>
                <spark.version>${spark353.version}</spark.version>
                <delta.core.artifactory>${delta.core.artifactory.post35}</delta.core.artifactory>
                <delta.core.version>${delta33x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark35x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark354</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>354</value>
                </property>
            </activation>
            <properties>
                <buildver>354</buildver>
                <spark.version>${spark354.version}</spark.version>
                <delta.core.artifactory>${delta.core.artifactory.post35}</delta.core.artifactory>
                <delta.core.version>${delta33x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark35x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark355</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>355</value>
                </property>
            </activation>
            <properties>
                <buildver>355</buildver>
                <spark.version>${spark355.version}</spark.version>
                <delta.core.artifactory>${delta.core.artifactory.post35}</delta.core.artifactory>
                <delta.core.version>${delta33x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark35x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark356</id>
            <activation>
                <property>
                    <name>buildver</name>
                    <value>356</value>
                </property>
            </activation>
            <properties>
                <buildver>356</buildver>
                <spark.version>${spark356.version}</spark.version>
                <delta.core.artifactory>${delta.core.artifactory.post35}</delta.core.artifactory>
                <delta.core.version>${delta33x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark35x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>spark357</id>
            <activation>
                <activeByDefault>true</activeByDefault>
                <property>
                    <name>buildver</name>
                    <value>357</value>
                </property>
            </activation>
            <properties>
                <buildver>357</buildver>
                <spark.version>${spark357.version}</spark.version>
                <delta.core.artifactory>${delta.core.artifactory.post35}</delta.core.artifactory>
                <delta.core.version>${delta33x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <iceberg.spark.artifact>${iceberg.spark35x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <!--
              Define a profile to be used for release. This profile
              should set the properties accordingly to make the jars suitable for production.
              For instance, set the elide-level to release to remove scala asserts.

              Release jobs should be using that Profile.
            -->
            <id>release</id>
            <properties>
                <buildver>357</buildver>
                <spark.version>${spark357.version}</spark.version>
                <delta.core.artifactory>${delta.core.artifactory.post35}</delta.core.artifactory>
                <delta.core.version>${delta33x.version}</delta.core.version>
                <hadoop.version>3.3.6</hadoop.version>
                <elide.level>${elide.level.release}</elide.level>
                <iceberg.spark.artifact>${iceberg.spark35x.artifact}</iceberg.spark.artifact>
                <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
            </properties>
        </profile>
        <profile>
            <id>scala213</id>
            <properties>
                <scala.binary.version>2.13</scala.binary.version>
                <scala.version>2.13.12</scala.version>
                <scalatest.version>3.2.19</scalatest.version>
                <scala.wconf.args>-Wconf:cat=deprecation:s,cat=lint-adapted-args:e</scala.wconf.args>
            </properties>
            <build>
                <plugins>
                    <plugin>
                        <groupId>net.alchim31.maven</groupId>
                        <artifactId>scala-maven-plugin</artifactId>
                        <configuration>
                            <encoding>${platform-encoding}</encoding>
                            <scalaVersion>${scala.version}</scalaVersion>
                            <checkMultipleScalaVersions>true</checkMultipleScalaVersions>
                            <failOnMultipleScalaVersions>true</failOnMultipleScalaVersions>
                            <recompileMode>incremental</recompileMode>
                            <args>
                                <arg>-unchecked</arg>
                                <arg>-deprecation</arg>
                                <arg>-feature</arg>
                                <arg>-explaintypes</arg>
                                <!-- Skip -Yno-adapted-args for Scala 2.13 (not supported) -->
                                <arg>-Ywarn-unused:imports,locals,patvars,privates</arg>
                                <arg>-Xlint:missing-interpolator</arg>
                                <arg>-Xfatal-warnings</arg>
                                <arg>${scala.wconf.args}</arg>
                                <arg>-Xelide-below</arg>
                                <arg>${elide.level}</arg>
                            </args>
                            <jvmArgs>
                                <jvmArg>-Xms1024m</jvmArg>
                                <jvmArg>-Xmx1024m</jvmArg>
                            </jvmArgs>
                            <addJavacArgs>${scala.javac.args}</addJavacArgs>
                        </configuration>
                    </plugin>
                </plugins>
            </build>
        </profile>
    </profiles>
    <properties>
        <java.version>1.8</java.version>
        <platform-encoding>UTF-8</platform-encoding>
        <!-- Define all spark versions we might want to build against -->
        <spark311.version>3.1.1</spark311.version>
        <spark312.version>3.1.2</spark312.version>
        <spark313.version>3.1.3</spark313.version>
        <spark314.version>3.1.4-SNAPSHOT</spark314.version>
        <spark320.version>3.2.0</spark320.version>
        <spark321.version>3.2.1</spark321.version>
        <spark322.version>3.2.2</spark322.version>
        <spark323.version>3.2.3</spark323.version>
        <spark324.version>3.2.4</spark324.version>
        <spark325.version>3.2.5-SNAPSHOT</spark325.version>
        <spark330.version>3.3.0</spark330.version>
        <spark331.version>3.3.1</spark331.version>
        <spark332.version>3.3.2</spark332.version>
        <spark333.version>3.3.3</spark333.version>
        <spark334.version>3.3.4</spark334.version>
        <spark335.version>3.3.5-SNAPSHOT</spark335.version>
        <spark340.version>3.4.0</spark340.version>
        <spark341.version>3.4.1</spark341.version>
        <spark342.version>3.4.2</spark342.version>
        <spark343.version>3.4.3</spark343.version>
        <spark344.version>3.4.4</spark344.version>
        <spark350.version>3.5.0</spark350.version>
        <spark351.version>3.5.1</spark351.version>
        <spark352.version>3.5.2</spark352.version>
        <spark353.version>3.5.3</spark353.version>
        <spark354.version>3.5.4</spark354.version>
        <spark355.version>3.5.5</spark355.version>
        <spark356.version>3.5.6</spark356.version>
        <spark357.version>3.5.7</spark357.version>
        <!-- Define default versions for other dependencies -->
        <scala.binary.version>2.12</scala.binary.version>
        <scala.plugin.version>4.3.0</scala.plugin.version>
        <scalatest-maven-plugin.version>2.0.2</scalatest-maven-plugin.version>
        <maven.clean.plugin.version>3.2.0</maven.clean.plugin.version>
        <scala.version>2.12.15</scala.version>
        <snakeyaml.version>2.0</snakeyaml.version>
        <scallop.version>3.5.1</scallop.version>
        <scalatest.version>3.2.19</scalatest.version>
        <spark.version.classifier>spark${buildver}</spark.version.classifier>
        <target.classifier>${spark.version.classifier}</target.classifier>
        <maven.jar.plugin.version>3.2.0</maven.jar.plugin.version>
        <maven.source.plugin.version>3.3.1</maven.source.plugin.version>
        <flatten.maven.plugin.version>1.6.0</flatten.maven.plugin.version>
        <maven.artifact.version>3.9.0</maven.artifact.version>
        <scala.javac.args>-Xlint:all,-serial,-path,-try</scala.javac.args>
        <rapids.shade.package>com.nvidia.shaded.spark</rapids.shade.package>
        <benchmarks.checkpoints>noOp</benchmarks.checkpoints>
        <jsoup.version>1.16.1</jsoup.version>
        <scala.collection.compat.version>2.12.0</scala.collection.compat.version>
        <!-- properties used for Iceberg (mainly for testing) -->
        <!--
          Iceberg artifacts are suffixed by the spark major version they are built against and the
          scala binaries.
        -->
        <iceberg.spark35x.artifact>iceberg-spark-runtime-3.5_${scala.binary.version}</iceberg.spark35x.artifact>
        <iceberg.spark34x.artifact>iceberg-spark-runtime-3.4_${scala.binary.version}</iceberg.spark34x.artifact>
        <iceberg.spark33x.artifact>iceberg-spark-runtime-3.3_${scala.binary.version}</iceberg.spark33x.artifact>
        <iceberg.spark32x.artifact>iceberg-spark-runtime-3.2_${scala.binary.version}</iceberg.spark32x.artifact>
        <!-- Default Iceberg properties -->
        <iceberg.spark.artifact>${iceberg.spark35x.artifact}</iceberg.spark.artifact>
        <iceberg.spark.runtime.version>${iceberg.spark.runtime.java.8}</iceberg.spark.runtime.version>
        <!--
             Iceberg 1.7.0+ requires Java 11+.
             Since we use jdk 1.8 as default, we need to use an older version of Iceberg.
             For Spark-3.2, the version is 1.4.3
        -->
        <iceberg.spark32x.runtime.version>1.4.3</iceberg.spark32x.runtime.version>
        <iceberg.spark.runtime.java.8>1.6.1</iceberg.spark.runtime.java.8>
        <iceberg.spark.runtime.java.11>1.10.0</iceberg.spark.runtime.java.11>
        <!-- properties used for DeltaLake -->
        <delta10x.version>1.0.1</delta10x.version>
        <delta11x.version>1.1.0</delta11x.version>
        <delta12x.version>1.2.1</delta12x.version>
        <delta20x.version>2.0.2</delta20x.version>
        <delta21x.version>2.1.1</delta21x.version>
        <delta22x.version>2.2.0</delta22x.version>
        <delta23x.version>2.3.0</delta23x.version>
        <delta24x.version>2.4.0</delta24x.version>
        <delta31x.version>3.1.0</delta31x.version>
        <delta33x.version>3.3.2</delta33x.version>
        <delta.core.version>${delta24x.version}</delta.core.version>
        <delta.core.artifactory>delta-core_${scala.binary.version}</delta.core.artifactory>
        <delta.core.artifactory.post35>delta-spark_${scala.binary.version}</delta.core.artifactory.post35>
        <!-- environment properties -->
        <!--
            Set the elide level to 2001 to disable Scala asserts in source code by default.
            The assertion value in elide is ASSERTION = 2000.
            For building a release, the elide.level should be set to 2001.
            For debugging/development and testing, the elide.level should be set to WARNING.
        -->
        <elide.level.release>2001</elide.level.release>
        <elide.level.dev>MINIMUM</elide.level.dev>
        <!--
            The default elide level to dev A build for RELEASE should overwrite
            that to disable non-production code (i.e., Scala asserts).
        -->
        <elide.level>${elide.level.dev}</elide.level>
        <maven-compiler-plugin.version>3.11.0</maven-compiler-plugin.version>
        <maven.compiler.source>${java.version}</maven.compiler.source>
        <maven.compiler.target>${java.version}</maven.compiler.target>
        <maven.scaladoc.skip>false</maven.scaladoc.skip>
        <maven.scalastyle.skip>false</maven.scalastyle.skip>
        <project.build.sourceEncoding>${platform-encoding}</project.build.sourceEncoding>
        <project.reporting.sourceEncoding>${platform-encoding}</project.reporting.sourceEncoding>
        <project.reporting.outputEncoding>${platform-encoding}</project.reporting.outputEncoding>
        <!--
            Project-level defaults so builds with -Pscala213 (and no -Dbuildver)
            still resolve Spark/Hadoop versions. These match the current default
            Spark profile (spark356). Individual spark3xx profiles will override.
        -->
        <spark.version>${spark357.version}</spark.version>
        <hadoop.version>3.3.6</hadoop.version>
        <!-- Testing properties -->
        <!-- define the location to generate the golden-sets -->
        <tools.qual.test.generate.golden.dir>${project.basedir}/golden-sets/${buildver}/qual</tools.qual.test.generate.golden.dir>
        <!-- enable generation of golden-sets -->
        <tools.qual.test.generate.golden.enable>false</tools.qual.test.generate.golden.enable>
        <tools.qual.test.keep.per_app.summary>false</tools.qual.test.keep.per_app.summary>
        <!-- enable/disable cleanup of temporary directories created during testing -->
        <tools.test.cleanup.tmp.dir>true</tools.test.cleanup.tmp.dir>
        <!-- Scala compiler warning configuration - overridden for Scala 2.13 -->
        <scala.wconf.args>-Wconf:cat=lint-adapted-args:e</scala.wconf.args>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
            <scope>compile</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
            <exclusions>
                <exclusion>
                    <!-- Exclude slf4j-reload4j from Hadoop-Client -->
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-reload4j</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.rogach</groupId>
            <artifactId>scallop_${scala.binary.version}</artifactId>
            <version>${scallop.version}</version>
        </dependency>
        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.binary.version}</artifactId>
            <version>${scalatest.version}</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.scala-lang.modules</groupId>
            <artifactId>scala-collection-compat_${scala.binary.version}</artifactId>
            <version>${scala.collection.compat.version}</version>
        </dependency>
        <dependency>
            <groupId>com.github.tototoshi</groupId>
            <artifactId>scala-csv_${scala.binary.version}</artifactId>
            <version>2.0.0</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.iceberg</groupId>
            <artifactId>${iceberg.spark.artifact}</artifactId>
            <version>${iceberg.spark.runtime.version}</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <!-- Required for processing yaml files in AutoTuner -->
            <groupId>org.yaml</groupId>
            <artifactId>snakeyaml</artifactId>
            <version>${snakeyaml.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.maven</groupId>
            <artifactId>maven-artifact</artifactId>
            <version>${maven.artifact.version}</version>
        </dependency>
        <dependency>
            <!-- jsoup HTML parser library @ https://jsoup.org/ -->
            <groupId>org.jsoup</groupId>
            <artifactId>jsoup</artifactId>
            <version>${jsoup.version}</version>
        </dependency>
        <!-- Dependencies used in testing -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <!-- add delta-lake to test against delta-lake write optimizations -->
            <groupId>io.delta</groupId>
            <artifactId>${delta.core.artifactory}</artifactId>
            <version>${delta.core.version}</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <!-- add hive jars to test hive Ops -->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <finalName>rapids-4-spark-tools_${scala.binary.version}-${project.version}</finalName>
        <resources>
            <resource>
                <directory>${project.basedir}/src/main/resources</directory>
                <filtering>true</filtering>
            </resource>
            <resource>
                <directory>${project.basedir}/..</directory>
                <targetPath>META-INF</targetPath>
                <includes>
                    <!-- The NOTICE will be taken care of by the antrun task below -->
                    <include>LICENSE</include>
                </includes>
            </resource>
        </resources>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-resources-plugin</artifactId>
                <version>3.3.1</version>
                <configuration>
                    <encoding>${project.build.sourceEncoding}</encoding>
                    <propertiesEncoding>${project.build.sourceEncoding}</propertiesEncoding>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.scalatest</groupId>
                <artifactId>scalatest-maven-plugin</artifactId>
                <version>${scalatest-maven-plugin.version}</version>
                <configuration>
                    <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
                    <junitxml>.</junitxml>
                    <filereports>scala-test-output.txt</filereports>
                    <stderr/>
                    <systemProperties>
                        <java.awt.headless>true</java.awt.headless>
                        <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
                        <spark.ui.enabled>false</spark.ui.enabled>
                        <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
                        <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
                    </systemProperties>
                </configuration>
                <executions>
                    <execution>
                        <id>test</id>
                        <goals>
                            <goal>test</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>flatten-maven-plugin</artifactId>
                <version>${flatten.maven.plugin.version}</version>
                <configuration>
                    <updatePomFile>true</updatePomFile>
                    <flattenMode>resolveCiFriendliesOnly</flattenMode>
                    <pomElements>
                        <artifactId>resolve</artifactId>
                    </pomElements>
                </configuration>
                <executions>
                    <execution>
                        <id>flatten</id>
                        <phase>process-resources</phase>
                        <goals>
                            <goal>flatten</goal>
                        </goals>
                    </execution>
                    <execution>
                        <id>flatten.clean</id>
                        <phase>clean</phase>
                        <goals>
                            <goal>clean</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.4</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <minimizeJar>true</minimizeJar>
                            <artifactSet>
                                <includes>
                                    <include>org.rogach:scallop_${scala.binary.version}</include>
                                    <include>org.yaml:snakeyaml</include>
                                    <include>org.apache.maven:maven-artifact</include>
                                    <include>org.jsoup:jsoup</include>
                                </includes>
                            </artifactSet>
                            <relocations>
                                <relocation>
                                    <pattern>org.yaml</pattern>
                                    <shadedPattern>${rapids.shade.package}.org.yaml</shadedPattern>
                                </relocation>
                                <relocation>
                                    <pattern>org.apache.maven.artifact</pattern>
                                    <shadedPattern>${rapids.shade.package}.org.apache.maven.artifact</shadedPattern>
                                </relocation>
                                <relocation>
                                    <pattern>org.jsoup</pattern>
                                    <shadedPattern>${rapids.shade.package}.org.jsoup</shadedPattern>
                                </relocation>
                            </relocations>
                            <filters>
                                <filter>
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.MF</exclude>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                        <exclude>META-INF/*.md</exclude>
                                    </excludes>
                                </filter>
                                <filter>
                                    <artifact>org.apache.maven:maven-artifact</artifact>
                                    <excludes>
                                        <exclude>META-INF/LICENSE</exclude>
                                        <exclude>META-INF/NOTICE</exclude>
                                    </excludes>
                                </filter>
                                <filter>
                                    <artifact>org.jsoup:jsoup</artifact>
                                    <excludes>
                                        <exclude>META-INF/LICENSE</exclude>
                                        <exclude>META-INF/NOTICE</exclude>
                                        <exclude>META-INF/CHANGES</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                            <transformers>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>com.nvidia.spark.rapids.tool.profiling.ProfileMain</mainClass>
                                </transformer>
                            </transformers>
                            <createDependencyReducedPom>true</createDependencyReducedPom>
                            <finalName>rapids-4-spark-tools_${scala.binary.version}-${project.version}</finalName>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.rat</groupId>
                <artifactId>apache-rat-plugin</artifactId>
                <version>0.13</version>
            </plugin>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>${scala.plugin.version}</version>
                <executions>
                    <execution>
                        <id>eclipse-add-source</id>
                        <goals>
                            <goal>add-source</goal>
                        </goals>
                    </execution>
                    <execution>
                        <id>scala-compile-first</id>
                        <phase>process-resources</phase>
                        <goals>
                            <goal>compile</goal>
                        </goals>
                    </execution>
                    <execution>
                        <id>scala-test-compile-first</id>
                        <phase>process-test-resources</phase>
                        <goals>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                    <!-- build javadoc.jar -->
                    <execution>
                        <id>attach-scaladocs</id>
                        <phase>compile</phase>
                        <goals>
                            <goal>doc-jar</goal>
                        </goals>
                        <configuration>
                            <args>
                                <arg>-doc-external-doc:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar#https://docs.oracle.com/javase/8/docs/api/index.html</arg>
                                <arg>-doc-external-doc:${settings.localRepository}/org/scala-lang/scala-library/${scala.version}/scala-library-${scala.version}.jar#https://scala-lang.org/api/${scala.version}/</arg>
                                <arg>-doc-external-doc:${settings.localRepository}/org/apache/spark/spark-sql_${scala.binary.version}/${spark.version}/spark-sql_${scala.binary.version}-${spark.version}/.jar#https://spark.apache.org/docs/${spark.version}/api/scala/index.html</arg>
                            </args>
                            <scalaVersion>${scala.version}</scalaVersion>
                            <checkMultipleScalaVersions>true</checkMultipleScalaVersions>
                            <failOnMultipleScalaVersions>true</failOnMultipleScalaVersions>
                            <jvmArgs>
                                <jvmArg>-Xms1024m</jvmArg>
                                <jvmArg>-Xmx1024m</jvmArg>
                            </jvmArgs>
                            <addJavacArgs>-Xlint:all,-serial,-path,-try</addJavacArgs>
                        </configuration>
                    </execution>
                </executions>
                <configuration>
                    <encoding>${platform-encoding}</encoding>
                    <scalaVersion>${scala.version}</scalaVersion>
                    <checkMultipleScalaVersions>true</checkMultipleScalaVersions>
                    <failOnMultipleScalaVersions>true</failOnMultipleScalaVersions>
                    <recompileMode>incremental</recompileMode>
                    <args>
                        <arg>-unchecked</arg>
                        <arg>-deprecation</arg>
                        <arg>-feature</arg>
                        <arg>-explaintypes</arg>
                        <arg>-Yno-adapted-args</arg>
                        <arg>-Ywarn-unused:imports,locals,patvars,privates</arg>
                        <arg>-Xlint:missing-interpolator</arg>
                        <arg>-Xfatal-warnings</arg>
                        <arg>${scala.wconf.args}</arg>
                        <arg>-Xelide-below</arg>
                        <arg>${elide.level}</arg>
                    </args>
                    <jvmArgs>
                        <jvmArg>-Xms1024m</jvmArg>
                        <jvmArg>-Xmx1024m</jvmArg>
                    </jvmArgs>
                    <addJavacArgs>${scala.javac.args}</addJavacArgs>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-antrun-plugin</artifactId>
                <version>3.0.0</version>
                <!-- parent-pom only executions -->
                <inherited>false</inherited>
                <dependencies>
                    <!-- Include scalastyle for cross-compilation (only available for Scala 2.12) -->
                    <dependency>
                        <groupId>org.scalastyle</groupId>
                        <artifactId>scalastyle_2.12</artifactId>
                        <version>1.0.0</version>
                    </dependency>
                </dependencies>
                <executions>
                    <execution>
                        <id>copy-notice</id>
                        <goals>
                            <goal>run</goal>
                        </goals>
                        <phase>process-resources</phase>
                        <configuration>
                            <target>
                                <!-- copy NOTICE-binary to NOTICE -->
                                <copy
                                        todir="${project.build.directory}/classes/META-INF/"
                                        verbose="true">
                                    <fileset dir="${project.basedir}/..">
                                        <include name="NOTICE-binary"/>
                                    </fileset>
                                    <mapper type="glob" from="*-binary" to="*"/>
                                </copy>
                            </target>
                        </configuration>
                    </execution>
                    <execution>
                        <!--
                        This is an alternative implementation of the scalastyle check invocation,
                        a replacement for scalastyle-maven-plugin. It's motivated to address the following:
                        - All scala files are checked at once regardless of the module, so the developer
                        can focus on addressing violations without being distracted by the build issues
                        in-between
                        - We don't have to hardcode the source code roots added dynamically by other maven
                        plugins to the project
                        - The scalastyle launch cost is amortized across all modules
                        -->
                        <id>scalastyle-all-modules</id>
                        <phase>verify</phase>
                        <goals><goal>run</goal></goals>
                        <configuration>
                            <skip>${maven.scalastyle.skip}</skip>
                            <target>
                                <pathconvert property="scalastyle.dirs" pathsep=" ">
                                    <dirset dir="${project.basedir}" includes="**/src/main/scala"/>
                                    <dirset dir="${project.basedir}" includes="**/src/main/*/scala"/>
                                    <dirset dir="${project.basedir}" includes="**/src/test/scala"/>
                                    <dirset dir="${project.basedir}" includes="**/src/test/*/scala"/>
                                </pathconvert>
                                <echo>Checking scalastyle for all modules using following paths:
                                    ${scalastyle.dirs}
                                </echo>
                                <java classname="org.scalastyle.Main" failonerror="true">
                                    <arg line="--verbose false"/>
                                    <arg line="--warnings false"/>
                                    <arg line="--config ${project.basedir}/scalastyle-config.xml"/>
                                    <arg line="--xmlOutput ${project.basedir}/target/scalastyle-output.xml"/>
                                    <arg line="--inputEncoding UTF-8"/>
                                    <arg line="--xmlEncoding UTF-8"/>
                                    <arg line="${scalastyle.dirs}"/>
                                </java>
                            </target>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <version>${maven.jar.plugin.version}</version>
            </plugin>
            <!-- build sources.jar -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-source-plugin</artifactId>
                <version>${maven.source.plugin.version}</version>
                <executions>
                    <execution>
                        <id>attach-source</id>
                        <goals>
                            <goal>jar-no-fork</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>${maven-compiler-plugin.version}</version>
                <configuration>
                    <encoding>${platform-encoding}</encoding>
                    <source>${maven.compiler.source}</source>
                    <target>${maven.compiler.target}</target>
                </configuration>
            </plugin>
        </plugins>
    </build>
    <repositories>
        <repository>
            <id>apache-snapshots-repo</id>
            <url>https://repository.apache.org/content/repositories/snapshots/</url>
            <releases>
                <enabled>false</enabled>
            </releases>
            <snapshots>
                <enabled>true</enabled>
            </snapshots>
        </repository>
    </repositories>
</project>
