[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"

[project]
name = "spark-rapids-user-tools"
authors = [
  { name="NVIDIA Corporation" , email="spark-rapids-support@nvidia.com" },
]
description = "A simple wrapper process around cloud service providers to run tools for the RAPIDS Accelerator for Apache Spark."
readme = "README.md"
requires-python = ">=3.8,<3.12"
classifiers = [
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "License :: OSI Approved :: Apache Software License",
    "Operating System :: OS Independent",
]
dependencies = [
    # numpy-1.24.4 is the latest version to support python3.8. P.S: does not support python-3.12+
    "numpy<=1.24.4",
    "chevron==0.14.0",
    "fastprogress==1.0.3",
    "fastcore==1.7.10",
    "fire>=0.5.0",
    "pandas==1.4.3",
    "pyYAML>=6.0,<=7.0",
    # This is used to resolve env-variable in yaml files. It requires netween 5.0 and 6.0
    "pyaml-env==1.2.1",
    "tabulate==0.8.10",
    "importlib-resources==5.10.2",
    "requests==2.31.0",
    "packaging>=23.0",
    "certifi==2024.7.4",
    "idna==3.4",
    "urllib3==1.26.19",
    "beautifulsoup4==4.11.2",
    "pygments==2.15.0",
    # used to apply validator on objects and models. "2.9.2" contains from_json method.
    "pydantic==2.9.2",
    # used to help pylint understand pydantic
    "pylint-pydantic==0.3.0",
    # used for common API to access remote filesystems like local/s3/gcs/hdfs
    # pin to 16.1.0 which works for both numpy-1.0 and numpy-2.0
    "pyarrow==16.1.0",
    # used for ADLS filesystem implementation
    # Issue-568: use 12.17.0 as the new 12.18.0 causes an error in runtime
    "azure-storage-blob==12.17.0",
    "adlfs==2023.4.0",
    # used for spinner animation
    "progress==1.6",
    # used for model estimations
    "xgboost==2.0.3",
    # used for model interpretability
    "shap==0.44.1",
    # used for retrieving available memory on the host
    "psutil==5.9.8",
    # pyspark for distributed computing
    "pyspark==3.5.0",
]
dynamic=["entry-points", "version"]

[project.scripts]
spark_rapids_user_tools = "spark_rapids_pytools.wrapper:main"
spark_rapids = "spark_rapids_tools.cmdli.tools_cli:main"
spark_rapids_dev = "spark_rapids_tools.cmdli.dev_cli:main"

[tool.setuptools]
package-dir = {"" = "src"}
[tool.setuptools.packages.find]
where = ["src"]
[tool.setuptools.dynamic]
version = {attr = "spark_rapids_pytools.__version__"}
[tool.setuptools.package-data]
"*"= ["*.json", "*.yaml", "*.ms", "*.sh", "*.tgz", "*.properties"]
[tool.poetry]
repository = "https://github.com/NVIDIA/spark-rapids-tools/tree/main"
[project.optional-dependencies]
test = [
    "tox", 'pytest', 'cli_test_helpers', 'behave',
    # use flak-8 plugin for pydantic
    'flake8-pydantic',
    # use pylint specific version
    'pylint==3.2.7',
]
qualx = [
    "holoviews",
    "matplotlib",
    "optuna",
    "optuna-integration",
    "seaborn"
]
