# RAPIDS User Tools on AWS EMR

This is a guide for the RAPIDS tools for Apache Spark on AWS EMR. At the end
of this guide, the user will be able to run the RAPIDS tools to analyze the clusters and
the applications running on AWS EMR.


## Prerequisites

### 1.AWS CLI

- Install the AWS CLI version 2. Follow the instructions on [aws-cli-getting-started](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html)
- Set the configuration settings and credentials of the AWS CLI by creating `credentials` and `config`
  files as described in [aws-cli-configure-files](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html).
- In order to be able to run tools that require SSH on the EMR nodes (i.e., bootstrap), then:
  - make sure that you have SSH access to the cluster nodes; and
  - create a key pair using Amazon EC2 through the AWS CLI command `aws ec2 create-key-pair`
    as instructed in [aws-cli-create-key-pairs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/create-key-pairs.html).
  - the private `.pem` file then can be passed to the RAPIDS tools CLI if needed.

### 2.RAPIDS tools

- Spark event logs:
  - The RAPIDS tools can process Apache Spark CPU event logs from Spark 2.0 or higher (raw, .lz4, .lzf, .snappy, .zstd)
  - For `qualification`/`profiling` commands, the event logs need to be archived to an accessible S3 folder.

### 3.Install the package

- Install `spark-rapids-user-tools` with python [3.8, 3.10] using:
  - pip:  `pip install spark-rapids-user-tools`
  - wheel-file: `pip install <wheel-file>`
  - from source: `pip install -e .`
- verify the command is installed correctly by running
  ```bash
    spark-rapids-user-tools emr --help
  ```

### 4.Environment variables

Before running any command, you can set environment variables to specify configurations.
- RAPIDS variables have a naming pattern `RAPIDS_USER_TOOLS_*`:
  - `RAPIDS_USER_TOOLS_CACHE_FOLDER`: specifies the location of a local directory that the RAPIDS-cli uses to
    store and cache the downloaded resources. The default is `/tmp/rapids_user_tools_cache`.  Note that
    caching the resources locally has an impact on the total execution time of the command.
- For AWS CLI, some environment variables can be set and picked by the RAPIDS-user tools such as:
  `AWS_PROFILE`, `AWS_DEFAULT_REGION` and `AWS_CONFIG_FILE`. See the full list of variables in
  [aws-cli-configure-envvars](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html).

## Qualification command

### Local deployment

```
spark_rapids_user_tools emr qualification [options]
spark_rapids_user_tools emr qualification --help
```

The local deployment runs on the local development machine. It requires:
1. Installing and configuring the AWS CLI
2. Java 1.8+ development environment
3. Internet access to download JAR dependencies from mvn: `spark-*.jar`, `hadoop-aws-*.jar`, and `aws-java-sdk-bundle*.jar`
4. Dependencies are cached on the local disk to reduce the overhead of the download.

#### Command options

| Option               | Description                                                                                                                                                                                                                                                                                                                                                                              | Default                                                                                                             | Required |
|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|:--------:|
| **eventlogs**        | A comma seperated list of S3 urls pointing to event logs or S3 directory                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                 |     Y    |
| **cpu_cluster**      | The EMR-cluster on which the Apache Spark applications were executed. Accepted values are an EMR-cluster name, or a valid path to the cluster properties file (json format) generated by AWS CLI command `emr describe-cluster`                                                                                                                                                          | N/A                                                                                                                 |     Y    |
| **remote_folder**    | The S3 folder where the output of the wrapper's output is copied. If missing, the output will be available only on local disk                                                                                                                                                                                                                                                            | N/A                                                                                                                 |     N    |
| **gpu_cluster**      | The EMR-cluster on which the Spark applications is planned to be migrated. The argument can be an EMR-cluster or a valid path to the cluster's properties file (json format) generated by the AWS CLI `emr describe-cluster` command                                                                                                                                                     | The wrapper maps the EC2 machine instances of the original cluster into EC2 instances that support GPU acceleration |     N    |
| **local_folder**     | Local work-directory path to store the output and to be used as root directory for temporary folders/files.                                                                                                                                                                                                                                                                              | The current execution directory                                                                                     |     N    |
| **jvm_heap_size**    | The maximum heap size of the JVM in gigabytes                                                                                                                                                                                                                                                                                                                                            | 24                                                                                                                  |     N    |
| **profile**          | A named AWS profile that you can specify to get the settings/credentials of the AWS account                                                                                                                                                                                                                                                                                              | "default" if the env-variable `AWS_PROFILE` is not set                                                              |     N    |
| **tools_jar**        | Path to a bundled jar including RAPIDS tool. The path is a local filesystem, or remote S3 url                                                                                                                                                                                                                                                                                            | Downloads the latest rapids-tools_*.jar from mvn repo                                                               |     N    |
| **filter_apps**      | Filtering criteria of the applications listed in the final STDOUT table. Note that this filter does not affect the CSV report. List is one of (`NONE`, `recommended`, `savings`) where "NONE" means no filter applied. "recommended" lists all the apps that are either 'Recommended', or 'Strongly Recommended'. "savings" lists all the apps that have positive estimated GPU savings. | `savings`                                                                                                           |     N    |
| **verbose**          | True or False to enable verbosity to the wrapper script                                                                                                                                                                                                                                                                                                                                  | False if `RAPIDS_USER_TOOLS_LOG_DEBUG` is not set                                                                   |     N    |
| **rapids_options**** | A list of valid [Qualification tool options](../../core/docs/spark-qualification-tool.md#qualification-tool-option). Note that (`output-directory`, `platform`) flags are ignored, and that multiple "spark-property" is not supported.                                                                                                                                                  | N/A                                                                                                                 |     N    |

#### Use case scenario

A typical workflow to successfully run the `qualification` command in local mode is described as follows:

1. Store the Apache Spark event logs in S3 folder.
2. A user sets up his development machine:
   1. configures Java
   2. installs AWS CLI and configures the profile and the credentials to make sure the AWS CLI
      commands can access the S3 resources `LOGS_BUCKET`.
   3. installs `spark_rapids_user_tools`
3. If the results of the wrapper need to be stored on S3, then another s3 uri is required `REMOTE_FOLDER=s3://OUT_BUCKET/`
4. User defines the EMR-cluster on which the Spark application were running. Note that the cluster does not have to be
   active; but it has to be visible by the  AWS CLI (i.e., can run `aws emr describe-cluster`).
5. The following script runs qualification by passing an AWS profile and S3 remote directory to store the output:
   
   ```
   # define the wrapper cache directory if necessary
   export RAPIDS_USER_TOOLS_CACHE_FOLDER=my_cache_folder
   export EVENTLOGS=s3://LOGS_BUCKET/eventlogs/
   export CLUSTER_NAME=my-emr-cpu-cluster
   export REMOTE_FOLDER=s3://OUT_BUCKET/wrapper_output
   export MY_AWS_PROFILE=my-aws-profile
   
   spark_rapids_user_tools emr qualification \
      --eventlogs $EVENTLOGS \
      --cpu_cluster $CLUSTER_NAME \
      --profile $MY_AWS_PROFILE \
      --remote_folder $REMOTE_FOLDER
   ```
   The wrapper generates a unique-Id for each execution in the format of `qual_<YYYYmmddHHmmss>_<0x%08X>`
   The above command will generate a directory containing `qualification_summary.csv` in a addition to
   the actual folder of the RAPIDS Qualification tool. The directory will be mirrored to S3 path (`REMOTE_FOLDER`).

   ```
    ./qual_<YYYYmmddHHmmss>_<0x%08X>/qualification_summary.csv
    ./qual_<YYYYmmddHHmmss>_<0x%08X>/rapids_4_spark_qualification_output/
   ```


### Serverless deployment

This deployment option allows running the RAPIDS tools on an Amazon EMR-Serverless.
This option has the following advantages:
1. Scalability in handling large pool of event logs
2. Convenience for not requiring to spin a cluster.

Resources to get familiar with Amazon EMR-Serverless:
- [Getting started with Amazon EMR-Serverless](https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/getting-started.html)
- [EMRServerless-Spark jobs](https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/jobs-spark.html)

#### Prerequisites

Obtain EMR-Serverless job-arn

In order to submit a job on Amazon EMR-Serverless, you need to have a "_job role-arn_".
The CLI guide on [Getting started with Amazon EMR Serverless](https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/getting-started.html)
lists the following steps to obtain the _role-arn_ (dubbed _EMRServerlessS3RuntimeRole_):
1. Grant permissions to use EMR-Serverless
2. Prepare storage for EMR-Serverless to save the output of the execution and the event logs
3. Create a job runtime role:
   1. Create a trust policy to use for IAM-role (_EMRServerlessS3RuntimeRole_), and save role-ARN in the output
   2. Use the trust policy to create the IAM-role
   3. Create a policy file that defines the access to resources (event logs and jars if needed)
   4. Create an IAM policy using the policy file
   5. Attach the IAM policy to the job runtime role, and save the job role-arn
4. _Optional_: create EMR-Serverless application


#### Qualification options

```
spark_rapids_user_tools emr qualification [options] --mode=serverless
spark_rapids_user_tools emr qualification --mode=serverless --help
```

| Option               | Description                                                                                                                                                                                                                                                                                                                                                                              | Default                                                                                                             | Required |
|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|:--------:|
| **eventlogs**        | A comma seperated list of S3 urls pointing to event logs or S3 directory                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                 |     Y    |
| **cpu_cluster**      | The EMR-cluster on which the Apache Spark applications were executed. Accepted values are an EMR-cluster name, or a valid path to the cluster properties file (json format) generated by AWS CLI command `emr describe-cluster`                                                                                                                                                          | N/A                                                                                                                 |     Y    |
| **remote_folder**    | The S3 folder where the output is archived                                                                                                                                                                                                                                                                                                                                               | N/A                                                                                                                 |     Y    |
| **job_arn**          | The job runtime role-ARN                                                                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                 |     Y    |
| **app_id**           | The ID of the EMR-serverless application on which to run the job. Note that creating an EMR-serverless application takes a few minutes                                                                                                                                                                                                                                                   | The wrapper creates an EMR-serverless application that gets deleted at the end of the execution                     |     N    |
| **gpu_cluster**      | The EMR-cluster on which the Spark applications is planned to be migrated. The argument can be an EMR-cluster or a valid path to the cluster's properties file (json format) generated by the AWS CLI `emr describe-cluster` command                                                                                                                                                     | The wrapper maps the EC2 machine instances of the original cluster into EC2 instances that support GPU acceleration |     N    |
| **local_folder**     | Local work-directory path to store the output and to be used as root directory for temporary folders/files. This determines if the final output should be locally downloaded to the local development machine                                                                                                                                                                            | The wrapper creates a temporary directory and gets deleted at the end of execution.                                 |     N    |
| **profile**          | A named AWS profile that you can specify to get the settings/credentials of the AWS account.                                                                                                                                                                                                                                                                                             | "default" if the the env-variable `AWS_PROFILE` is not set                                                          |     N    |
| **tools_jar**        | Path to a bundled jar including RAPIDS tool. The path is a local filesystem, or remote S3 url                                                                                                                                                                                                                                                                                            | Downloads the latest rapids-tools_*.jar from mvn repo                                                               |     N    |
| **filter_apps**      | Filtering criteria of the applications listed in the final STDOUT table. Note that this filter does not affect the CSV report. List is one of (`NONE`, `recommended`, `savings`) where "NONE" means no filter applied. "recommended" lists all the apps that are either 'Recommended', or 'Strongly Recommended'. "savings" lists all the apps that have positive estimated GPU savings. | `savings`                                                                                                           |     N    |
| **verbose**          | True or False to enable verbosity to the wrapper script                                                                                                                                                                                                                                                                                                                                  | False if `RAPIDS_USER_TOOLS_LOG_DEBUG` is not set                                                                   |     N    |
| **rapids_options**** | A list of valid [Qualification tool options](../../core/docs/spark-qualification-tool.md#qualification-tool-option). Note that (`output-directory`, `platform`) flags are ignored, and that multiple "spark-property" is not supported.                                                                                                                                                  | N/A                                                                                                                 |     N    |

A typical workflow to successfully run the `qualification` command in serverless mode is described as follows:

1. Store the Apache Spark event logs in S3 folder.
2. A user sets up his development machine:
   1. installs AWS CLI and configures the profile and the credentials to make sure the AWS CLI
       commands can access the S3 resources `LOGS_BUCKET`.
   2. installs `spark_rapids_user_tools`
3. Another s3 uri is required `REMOTE_FOLDER=s3://OUT_BUCKET/` to store the output
4. User creates and sets a job runtime role as described in previous section.
5. User defines the EMR-cluster on which the Spark application were running. Note that the cluster does not have to be
   active; but it has to be visible by the  AWS CLI (i.e., can run `aws emr describe-cluster`).
6. The following script runs qualification as an EMR-serverless job by passing an AWS profile,
   S3 remote directory to store the output, and the job-ARN:

   ```
   # define the wrapper cache directory if necessary
   export RAPIDS_USER_TOOLS_CACHE_FOLDER=my_cache_folder
   export EVENTLOGS=s3://LOGS_BUCKET/eventlogs/
   export CLUSTER_NAME=my-emr-cpu-cluster
   export JOB_ARN="arn:aws:iam::[0-9]+:role/ROLE_NAME"
   export REMOTE_FOLDER=s3://OUT_BUCKET/wrapper_output
   export MY_AWS_PROFILE=my-aws-profile
   
   spark_rapids_user_tools emr qualification \
      --eventlogs $EVENTLOGS \
      --cpu_cluster $CLUSTER_NAME \
      --profile $MY_AWS_PROFILE \
      --remote_folder $REMOTE_FOLDER \
      --job_arn $JOB_ARN \
      --mode=serverless
   ```
   The wrapper generates a unique-Id for each execution in the format of `qual_<YYYYmmddHHmmss>_<0x%08X>`
   The above command will generate an S3 directory containing `qualification_summary.csv` in addition to
   the actual folder of the RAPIDS Qualification tool. If `local_directory` is passed as an argument, then
   the wrapper mirrors the remote folder on local disk.

   ```
    $REMOTE_FOLDER/qual_<YYYYmmddHHmmss>_<0x%08X>/qualification_summary.csv
    $REMOTE_FOLDER/qual_<YYYYmmddHHmmss>_<0x%08X>/rapids_4_spark_qualification_output/
   ```

## Bootstrap command

```
spark_rapids_user_tools emr bootstrap [options]
spark_rapids_user_tools emr bootstrap --help
```
