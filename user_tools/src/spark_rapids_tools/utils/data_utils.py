# Copyright (c) 2025, NVIDIA CORPORATION.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Utility functions to use common data handling such as reading an opening files generated by
core tools."""

import os
import json
from dataclasses import dataclass
from typing import Optional, List, Callable, Any, Union, IO

import pandas as pd

from spark_rapids_tools import CspPathT


@dataclass
class LoadDFResult(object):
    """
    A class that represents the result of a download task. It contains the following information:
    :param f_path: The path where the CSV located. Notice that we use a plain string and not
                   a CspPath as we want to keep the class lightWeight in case we face performance
                   issues related to keeping CspPath objects in memory.
    :param df: The dataframe that was loaded from the file.
    :param success: Whether the loading is successful.
    :param fallen_back: Was the DF resulting DF created from a fallBack to default value, or was it
                        loaded actually from the file.
    :param load_error: The error that occurred during the loading if any.
    """
    f_path: str
    df: Optional[pd.DataFrame]
    success: bool
    fallen_back: bool
    load_error: Optional[Exception] = None

    def df_to_dict(self) -> List[dict[str, str]]:
        return DataUtils.convert_df_to_dict(self.df)

    def get_fail_cause(self) -> Optional[Exception]:
        if self.load_error:
            return self.load_error.__cause__
        return None


class DataUtils:
    """
    Utility functions to use common data handling such as reading an opening CSV files.
    """

    @staticmethod
    def convert_df_to_dict(df: pd.DataFrame) -> List[dict[str, str]]:
        """
        Util function to convert a dataframe to a list of dictionaries. This is convenient when
        the data is serialized to objects for better maintainability in the code.
        :param df:
        :return:
        """
        return df.to_dict(orient='records')

    @staticmethod
    def read_dataframe(
            filepath_or_buffer: Union[str, bytes, os.PathLike, IO[Any]],
            map_columns: Optional[dict] = None,
            **read_csv_kwargs) -> pd.DataFrame:
        """
        Read a pandas DataFrame from a file path or buffer with optional column mapping and default value.

        :param filepath_or_buffer: A file path or a file-like object containing the CSV data.
                             Can be a string path, path-like object, or file-like object.
        :param map_columns: Optional dictionary to rename columns (keys: old names, values: new names).
        :param read_csv_kwargs: Additional arguments to pass to pandas.read_csv(). See the full list
               of arguments in the API reference https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html.

        :return:
            pd.DataFrame: The loaded DataFrame, or the default DataFrame if reading fails and default is provided.
        :raises:
            Exception: If reading fails and no default is provided, the original exception is re-raised.

        Examples:
        >>> # load dataframe with semicolon separator
        >>> df_1 = DataUtils.read_dataframe('data.csv', map_columns={'old_name': 'new_name'})
        >>> # load dataframe with semicolon separator
        >>> df_2 = DataUtils.read_dataframe('data.csv', map_columns={'old_name': 'new_name'}, sep=';')
        >>> # load dataframe with selected columns
        >>> df_3 = DataUtils.read_dataframe('data.csv', usecols=['col1', 'col2'])
        """
        # TODO: add support for JSON files as part of read_dataframe and merge load_pd_df and load_pd_df_from_json
        try:
            df = pd.read_csv(filepath_or_buffer, **read_csv_kwargs)
            if map_columns:
                df = df.rename(columns=map_columns)
            return df
        except Exception as e:
            raise RuntimeError(f'Failed to read dataframe from {filepath_or_buffer} â€” {e}') from e

    @staticmethod
    def load_pd_df(f_path: Union[str, CspPathT],
                   default_cb: Optional[Callable[[], pd.DataFrame]] = None,
                   map_columns: Optional[dict] = None,
                   read_csv_kwargs: Optional[dict[str, Any]] = None) -> LoadDFResult:
        """
        :param f_path: the file path to load
        :param map_columns: optional dictionary to map column names
        :param default_cb: an optional function to construct a default Dataframe in case of the
               file is not found, or error
        :param read_csv_kwargs: optional dictionary for arguments to be forwarded to pandas read_csv
        :return: a LoadDFResult object holding information about the DF loading task
        """
        read_csv_kwargs = read_csv_kwargs or {}
        fallen_back = False
        load_error = None
        loaded_df = None
        success = False
        actual_path = str(f_path)
        try:
            if isinstance(f_path, str):
                # this is csPath that we need to check if it is remote or local.
                # if it is local, then we better use pandas_directly to avoid any performance issues.
                loaded_df = DataUtils.read_dataframe(f_path, map_columns=map_columns, **read_csv_kwargs)
            else:
                with f_path.open_input_stream() as fis:
                    loaded_df = DataUtils.read_dataframe(fis, map_columns=map_columns, **read_csv_kwargs)
            success = loaded_df is not None
        except Exception as e:  # pylint: disable=broad-except
            load_error = e
            loaded_df = None
        if loaded_df is None and default_cb:
            loaded_df = default_cb()
            fallen_back = True
            success = True
        return LoadDFResult(
            f_path=actual_path,
            df=loaded_df,
            success=success,
            fallen_back=fallen_back,
            load_error=load_error)

    @staticmethod
    def load_pd_df_from_json(f_path: Union[str, CspPathT],
                             default_cb: Optional[Callable[[], pd.DataFrame]] = None,
                             map_columns: Optional[dict] = None) -> LoadDFResult:
        """
        Load a pandas DataFrame from a JSON file using json_normalize.

        :param f_path: the file path to load
        :param default_cb: an optional function to construct a default DataFrame in case of the
               file is not found, or error
        :param map_columns: optional dictionary to map column names
        :return: a LoadDFResult object holding information about the DF loading task
        """
        fallen_back = False
        load_error = None
        loaded_df = None
        success = False
        actual_path = str(f_path)
        try:
            if isinstance(f_path, str):
                with open(f_path, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                loaded_df = pd.json_normalize(json_data) if json_data else pd.DataFrame()
            else:
                with f_path.open_input_stream() as fis:
                    json_data = json.load(fis)
                loaded_df = pd.json_normalize(json_data) if json_data else pd.DataFrame()
            # Apply column mapping if provided
            if loaded_df is not None and not loaded_df.empty and map_columns:
                loaded_df = loaded_df.rename(columns=map_columns)
            success = loaded_df is not None
        except Exception as e:  # pylint: disable=broad-except
            load_error = e
            loaded_df = None
        if loaded_df is None and default_cb:
            loaded_df = default_cb()
            fallen_back = True
            success = True
        return LoadDFResult(
            f_path=actual_path,
            df=loaded_df,
            success=success,
            fallen_back=fallen_back,
            load_error=load_error)
